{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais e sua Implementação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ruasgar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-Processamento básico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/final_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence', 'news_link', 'outlet', 'topic', 'type', 'group_id',\n",
       "       'num_sent', 'Label_bias', 'Label_opinion', 'article', 'biased_words4',\n",
       "       'full_article'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A priori, vamos considerar como tokens todas as palavras que:\n",
    "    - tenham mais do que 3 letras\n",
    "    - não possuam números\n",
    "    - não estejam nas stopwords\n",
    "    \n",
    "- Vamos remover os acentos e, mediante a escolha do usuário, aplicar um stemmer.\n",
    "- Visando o mapeamento das palavras para números inteiros, vamos utilizar o padrão de marcar alguns tokens especiais:\n",
    "    - 0: padding (completar o tamanho das representações)\n",
    "    - 1: Start-of-Sequence\n",
    "    - 2: tokens desconhecidos, necessários para novas palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [youtube, making, clear, birtherism, platform,...\n",
       "1    [increasingly, bitter, dispute, american, wome...\n",
       "2    [may, humanitarian, crisis, driving, vulnerabl...\n",
       "3    [professor, teaches, climate, change, classes,...\n",
       "4    [world, antidoping, agency, tuesday, said, rus...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_pattern = r'\\d+(\\.\\d+)?'\n",
    "solo_quotations_pattern = pattern = r\"^(?:' '|\\\\\" \"|\\`\\`)$\"\n",
    "\n",
    "remove_quotation = r\"'\\b|\\b'\\s|\\s'\\b|\\\"\\b|\\b\\\"\\s|\\s\\\"\\b|``\\b|\\b``\\s|\\s``\\b\" # como em \"America is Great\"\n",
    "remove_symbols_only = r'\\b[^\\w\\s]+\\b' # como em '--'\n",
    "remove_symbols_if_aside = r'\\b[^\\w\\s]|[^\\w\\s]\\b' # como em 'dog-'\n",
    "\n",
    "remove_patterns = remove_quotation,remove_symbols_if_aside, remove_symbols_only\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # talvez o set deixe mais rápido\n",
    "\n",
    "def preprocess_sentences(df):\n",
    "    \n",
    "    prepped_sentences = []\n",
    "\n",
    "    for s in df['sentence']:\n",
    "        # Remove acentos e põe tudo para lower case\n",
    "        s = unidecode(s).lower()\n",
    "        tokens = word_tokenize(s,language=\"english\")\n",
    "\n",
    "        tokens = [\n",
    "            re.sub('|'.join(remove_patterns),'',t)\n",
    "            for t in tokens \n",
    "            if not bool(re.search(digit_pattern,t))\n",
    "            and t not in string.punctuation\n",
    "            and t not in stop_words\n",
    "        ]\n",
    "\n",
    "        tokens = [\n",
    "            t for t in tokens\n",
    "            if not bool(re.match(solo_quotations_pattern,t)) \n",
    "            and len(t) >= 3\n",
    "        ]\n",
    "        \n",
    "        prepped_sentences.append(tokens)\n",
    "\n",
    "    df['sentence'] = prepped_sentences\n",
    "\n",
    "    return df \n",
    "\n",
    "\n",
    "df = preprocess_sentences(df)\n",
    "df['sentence'].head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
