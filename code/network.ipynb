{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais\n",
    "\n",
    "Esse notebook tratará da implementação, do treino e da avaliação inicial de redes neurais recorrentes (especificamente de células LSTM) na busca pela solução do seguinte problema de classificação: identificação de de viés político em textos. O tamanho desses textos é algo a ser arbitrado e até mesmo utilizado como comparação: podemos utilizar somente as sentenças que foram marcadas por conter viés, ou mesmo o texto na íntegra da notícia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import ast\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit \n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Carregamento do dataset e do vocabulário, totalmente perfilados pelo notebook de pré-processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3378\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn [124], line 2\u001b[0m\n    data[['sentence','article','biased_words4']] = data[['sentence','article','biased_words4']].apply(lambda x:x.apply(ast.literal_eval))\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:9433\u001b[0m in \u001b[1;35mapply\u001b[0m\n    return op.apply().__finalize__(self, method=\"apply\")\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py:678\u001b[0m in \u001b[1;35mapply\u001b[0m\n    return self.apply_standard()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py:798\u001b[0m in \u001b[1;35mapply_standard\u001b[0m\n    results, res_index = self.apply_series_generator()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py:814\u001b[0m in \u001b[1;35mapply_series_generator\u001b[0m\n    results[i] = self.f(v)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn [124], line 2\u001b[0m in \u001b[1;35m<lambda>\u001b[0m\n    data[['sentence','article','biased_words4']] = data[['sentence','article','biased_words4']].apply(lambda x:x.apply(ast.literal_eval))\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/.local/lib/python3.8/site-packages/pandas/core/series.py:4626\u001b[0m in \u001b[1;35mapply\u001b[0m\n    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py:1025\u001b[0m in \u001b[1;35mapply\u001b[0m\n    return self.apply_standard()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py:1076\u001b[0m in \u001b[1;35mapply_standard\u001b[0m\n    mapped = lib.map_infer(\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32mpandas/_libs/lib.pyx:2834\u001b[0m in \u001b[1;35mpandas._libs.lib.map_infer\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/usr/lib/python3.8/ast.py:59\u001b[0m in \u001b[1;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string, mode='eval')\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m/usr/lib/python3.8/ast.py:47\u001b[0;36m in \u001b[0;35mparse\u001b[0;36m\n\u001b[0;31m    return compile(source, filename, mode, flags,\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<unknown>:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    0       [1, 3, 11, 12, 6, 13, 14, 15, 9, 16, 17, 18, 1...\u001b[0m\n\u001b[0m                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/final_dataset.csv')\n",
    "data[['sentence','article','biased_words4']] = data[['sentence','article','biased_words4']].apply(lambda x:x.apply(ast.literal_eval))\n",
    "data.loc[1,'sentence'] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence', 'outlet_alternet', 'outlet_breitbart', 'outlet_federalist',\n",
       "       'outlet_fox-news', 'outlet_huffpost', 'outlet_msnbc', 'outlet_reuters',\n",
       "       'outlet_usa-today', 'topic_abortion', 'topic_coronavirus',\n",
       "       'topic_elections-2020', 'topic_environment', 'topic_gender',\n",
       "       'topic_gun-control', 'topic_immigration',\n",
       "       'topic_international-politics-and-world-news', 'topic_middle-class',\n",
       "       'topic_sport', 'topic_student-debt', 'topic_trump-presidency',\n",
       "       'topic_vaccines', 'topic_white-nationalism', 'type_center', 'type_left',\n",
       "       'type_right', 'num_sent', 'Label_bias_Biased',\n",
       "       'Label_bias_No agreement', 'Label_bias_Non-biased', 'article',\n",
       "       'biased_words4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/vocabulary_table.pkl', 'rb') as f:\n",
    "    vocab_dict = pickle.load(f)\n",
    "\n",
    "keys = list(vocab_dict.keys())\n",
    "values = list(vocab_dict.values())\n",
    "n_oov = 5000\n",
    "\n",
    "vocab_table = tf.lookup.StaticVocabularyTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(keys, values, tf.string, tf.int64),\n",
    "    n_oov\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Separando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = ['Label_bias_Biased','Label_bias_No agreement', 'Label_bias_Non-biased']\n",
    "labels = data[label_columns]\n",
    "features = data.drop(label_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split(features, labels):\n",
    "    \"\"\" Retorna uma lista de tuplas contendo os datasets de features e de labels para cada segmento (treino, validação, teste) \"\"\"\n",
    "\n",
    "    # Treino-val e Teste\n",
    "    shuffle_train_test = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=892)\n",
    "    train_val_indexes, test_indexes = next(shuffle_train_test.split(features.values, labels.values))\n",
    "    train_val_df, train_val_labels = features.iloc[train_val_indexes], labels.iloc[train_val_indexes]\n",
    "    test_df, test_labels = features.iloc[test_indexes], labels.iloc[test_indexes]\n",
    "\n",
    "    # Treino e Validação\n",
    "    shuffle_train_validate = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=124)\n",
    "    train_indexes, validation_indexes = next(shuffle_train_validate.split(train_val_df.values,train_val_labels.values))\n",
    "    train_df, train_labels = features.iloc[train_indexes], labels.iloc[train_indexes]\n",
    "    validation_df, validation_labels = features.iloc[validation_indexes], labels.iloc[validation_indexes]\n",
    "\n",
    "\n",
    "    return [(train_df, train_labels), (validation_df, validation_labels), (test_df, test_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino:(1014, 29) ----- Validação: (338, 29) ------ Teste: (338, 29)\n"
     ]
    }
   ],
   "source": [
    "train, validation, test = train_valid_test_split(features,labels)\n",
    "\n",
    "train_df, train_labels = train\n",
    "validation_df, validation_labels = validation\n",
    "test_df, test_labels = test\n",
    "\n",
    "print(f\"Treino:{train_df.shape} ----- Validação: {validation_df.shape} ------ Teste: {test_df.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Estruturando arquitetura da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 01:32:20.659539: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-14 01:32:20.665815: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-14 01:32:20.667932: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "\n",
    "# parte textual\n",
    "text_input_length = len(train_df.loc[0,'article']) + len(train_df.loc[0,'biased_words4']) + len(train_df.loc[0,'sentence'])\n",
    "text_input = keras.layers.Input(shape=(text_input_length,))\n",
    "embedding_layer = keras.layers.Embedding(input_dim=int(vocab_table.size())+5000,\n",
    "                                         output_dim=embed_size,\n",
    "                                         mask_zero=True)(text_input)\n",
    "\n",
    "embedding_output = keras.layers.Bidirectional(keras.layers.LSTM(embed_size))(embedding_layer) \n",
    "\n",
    "\n",
    "# outras features \n",
    "feature_input_length = len(train_df.columns) - 3\n",
    "feature_input = keras.layers.Input(shape=(feature_input_length,))\n",
    "\n",
    "concatenated = keras.layers.concatenate([embedding_output,feature_input], axis=-1)\n",
    "reshaped = keras.layers.Reshape((-1, concatenated.shape[-1]))(concatenated)  # Add time dimension\n",
    "gru_layer1 = keras.layers.GRU(64)(reshaped)\n",
    "output = keras.layers.Dense(3, activation=\"softmax\")(gru_layer1)\n",
    "\n",
    "model = tf.keras.Model(inputs=[text_input,feature_input], outputs=output)\n",
    "#model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = ['sentence','article','biased_words4']\n",
    "others = list(filter(lambda x: x if x not in text_cols else [],train_df.columns))\n",
    "\n",
    "train_df = pd.concat([train_df[text_cols], train_df[others]],axis=1).astype(int)\n",
    "validation_df = pd.concat([validation_df[text_cols], validation_df[others]],axis=1)\n",
    "test_df = pd.concat([test_df[text_cols], test_df[others]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3378\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn [117], line 2\u001b[0;36m\n\u001b[0;31m    eval(train_df.loc[0,'sentence'])\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    [ 1  3 11 12  6 13 14 15  9 16 17 18 19 20 21 22  0  0  0  0  0  0  0  0\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#train_df = train_df.explode('sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93     0\n",
       "526    0\n",
       "Name: outlet_alternet, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['outlet_alternet'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ruasgar/Bureau/text-bias-detection/code/network.ipynb Cell 16\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ruasgar/Bureau/text-bias-detection/code/network.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_df, train_labels, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(validation_df, validation_labels))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "model.fit(train_df, train_labels, batch_size=32, epochs=10, validation_data=(validation_df, validation_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
