{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais e sua Implementação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ruasgar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from unidecode import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/final_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence', 'news_link', 'outlet', 'topic', 'type', 'group_id',\n",
       "       'num_sent', 'Label_bias', 'Label_opinion', 'article', 'biased_words4',\n",
       "       'full_article'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Features Importantes\n",
    "- Label_bias (categórica): Indica se o texto foi classificado como enviesado, não-enviesado ou se não foi possível atingir um consenso quanto a classificação.\n",
    "- Label_opinion (categórica): Indica de que modo o viés se manifesta na percepção dos entrevistados; especificamente separando casos de exposição de opinião do autor ou com fatos que corroborem um viés. (Um pouco fuzzy demais, talvez?)\n",
    "- Biased_words(vetor): Indica as palavras marcadas como \"denunciantes\" da presença de viés. \n",
    "- Topic(categórica): Indica o assunto do texto, dentro das categorias PREENCHER AQUI   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Pré-Processamento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Básico textual (remoção de stopwords, filtro de expressões com números, etc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A priori, vamos considerar como tokens todas as palavras que:\n",
    "    - tenham mais do que 3 letras\n",
    "    - não possuam números\n",
    "    - não estejam nas stopwords do inglês\n",
    "    \n",
    "- Vamos remover os acentos e, mediante a escolha do usuário, aplicar um stemmer.\n",
    "- Visando o mapeamento das palavras para números inteiros, vamos utilizar o padrão de marcar alguns tokens especiais:\n",
    "    - 0: padding (completar o tamanho das representações)\n",
    "    - 1: Start-of-Sequence\n",
    "    - 2: tokens desconhecidos, necessários para novas palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [youtube, making, clear, birtherism, platform,...\n",
       "1    [increasingly, bitter, dispute, american, wome...\n",
       "2    [may, humanitarian, crisis, driving, vulnerabl...\n",
       "3    [professor, teaches, climate, change, classes,...\n",
       "4    [world, antidoping, agency, tuesday, said, rus...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_pattern = r'\\d+(\\.\\d+)?'\n",
    "solo_quotations_pattern = pattern = r\"^(?:' '|\\\\\" \"|\\`\\`)$\"\n",
    "\n",
    "remove_quotation = r\"'\\b|\\b'\\s|\\s'\\b|\\\"\\b|\\b\\\"\\s|\\s\\\"\\b|``\\b|\\b``\\s|\\s``\\b\" # como em \"America is Great\"\n",
    "remove_symbols_only = r'\\b[^\\w\\s]+\\b' # como em '--'\n",
    "remove_symbols_if_aside = r'\\b[^\\w\\s]|[^\\w\\s]\\b' # como em 'dog-'\n",
    "\n",
    "remove_patterns = remove_quotation,remove_symbols_if_aside, remove_symbols_only\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # talvez o set deixe mais rápido\n",
    "\n",
    "def preprocess_sentences(df):\n",
    "    \n",
    "    prepped_sentences = []\n",
    "\n",
    "    for s in df['sentence']:\n",
    "        # Remove acentos e põe tudo para lower case\n",
    "        s = unidecode(s).lower()\n",
    "        tokens = word_tokenize(s,language=\"english\")\n",
    "\n",
    "        tokens = [\n",
    "            re.sub('|'.join(remove_patterns),'',t)\n",
    "            for t in tokens \n",
    "            if not bool(re.search(digit_pattern,t))\n",
    "            and t not in string.punctuation\n",
    "            and t not in stop_words\n",
    "        ]\n",
    "\n",
    "        tokens = [\n",
    "            t for t in tokens\n",
    "            if not bool(re.match(solo_quotations_pattern,t)) \n",
    "            and len(t) >= 3\n",
    "        ]\n",
    "\n",
    "        prepped_sentences.append(tokens)\n",
    "\n",
    "    df['sentence'] = prepped_sentences\n",
    "\n",
    "    return df \n",
    "\n",
    "df_functions_test_copy = df.copy()\n",
    "df_functions_test_copy = preprocess_sentences(df_functions_test_copy)\n",
    "df_functions_test_copy['sentence'].head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Stemmers\n",
    "\n",
    "- Temos o objetivo de avaliar como diferentes estratégias de stemming afetam os resultados do treinamento. A função a seguir visa abstrair esse passo para um etapa separada de pré-processamento. \n",
    "- Serão testados os stemmer de Porter e de Lancaster, além da lematização via WordNet. Vamos também obter os resultados do modelo quando utilizados sem qualquer stemmer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stemmer(sentence, stemmer_type:str):\n",
    "\n",
    "    # Função a ser executada para cada sentença, com expectativa de uso em algum filtro, por exemplo\n",
    "\n",
    "    stemmer = {}\n",
    "    lemmatizer = {}\n",
    "\n",
    "    if stemmer_type == \"Porter\":\n",
    "        stemmer = PorterStemmer()\n",
    "        return [stemmer.stem(token) for token in sentence]\n",
    "\n",
    "    elif stemmer_type == \"Lancaster\":\n",
    "        stemmer = LancasterStemmer()\n",
    "        return [stemmer.stem(token) for token in sentence]\n",
    "    \n",
    "    else:  # \"Wordnet\":\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return [lemmatizer.stem(token) for token in sentence]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Aplicando stemmer para teste da função "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [youtub, make, clear, birther, platform, year,...\n",
       "1    [increasingli, bitter, disput, american, women...\n",
       "2    [may, humanitarian, crisi, drive, vulner, peop...\n",
       "3    [professor, teach, climat, chang, class, subje...\n",
       "4    [world, antidop, agenc, tuesday, said, russian...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_functions_test_copy['sentence'] = df_functions_test_copy['sentence'].apply(lambda x: apply_stemmer(x,\"Porter\"))\n",
    "df_functions_test_copy['sentence'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Separando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split(features, labels):\n",
    "    \"\"\" Retorna uma lista de tuplas contendo os datasets de features e de labels para cada segmento (treino, validação, teste) \"\"\"\n",
    "    \n",
    "    # Treino-val e Teste\n",
    "    shuffle_train_test = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=892)\n",
    "    train_val_indexes, test_indexes = next(shuffle_train_test.split(features.values, labels.values))\n",
    "    train_val_df, train_val_labels = features.iloc[train_val_indexes], labels.iloc[train_val_indexes]\n",
    "    test_df, test_labels = features.iloc[test_indexes], labels.iloc[test_indexes]\n",
    "\n",
    "    # Treino e Validação\n",
    "    shuffle_train_validate = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=124)\n",
    "    train_indexes, validation_indexes = next(shuffle_train_validate.split(train_val_df.values,train_val_labels.values))\n",
    "    train_df, train_labels = features.iloc[train_indexes], labels.iloc[train_indexes]\n",
    "    validation_df, validation_labels = features.iloc[validation_indexes], labels.iloc[validation_indexes]\n",
    "\n",
    "\n",
    "    return [(train_df, train_labels), (validation_df, validation_labels), (test_df, test_labels)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(848, 12)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, val, test = train_valid_test_split(df_functions_test_copy,df_functions_test_copy['type'])\n",
    "train[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Encoding (palavras)\n",
    "\n",
    "- Para representar as palavras, vamos criar um encoding com base na frequência apresentada no corpus.\n",
    "- Quando formos alimentar a rede neural, vamos utilizar uma camada de WordEmbedding do TensorFlow para representação do espaço \"textual\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ruasgar/Bureau/text-bias-detection/code/network.ipynb Cell 18\u001b[0m in \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ruasgar/Bureau/text-bias-detection/code/network.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m train_df_copy[\u001b[39m'\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m'\u001b[39m]:  \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ruasgar/Bureau/text-bias-detection/code/network.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     vocab\u001b[39m.\u001b[39mupdate(sentence)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ruasgar/Bureau/text-bias-detection/code/network.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m words \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant([word \u001b[39mfor\u001b[39;00m word,_ \u001b[39min\u001b[39;00m vocab])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ruasgar/Bureau/text-bias-detection/code/network.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m words_ids \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrange(\u001b[39mlen\u001b[39m(vocab), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint64)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ruasgar/Bureau/text-bias-detection/code/network.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m vocab_init \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mlookup\u001b[39m.\u001b[39mKeyValueTensorInitializer(words,words_ids)\n",
      "\u001b[1;32m/home/ruasgar/Bureau/text-bias-detection/code/network.ipynb Cell 18\u001b[0m in \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ruasgar/Bureau/text-bias-detection/code/network.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m train_df_copy[\u001b[39m'\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m'\u001b[39m]:  \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ruasgar/Bureau/text-bias-detection/code/network.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     vocab\u001b[39m.\u001b[39mupdate(sentence)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ruasgar/Bureau/text-bias-detection/code/network.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m words \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant([word \u001b[39mfor\u001b[39;00m word,_ \u001b[39min\u001b[39;00m vocab])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ruasgar/Bureau/text-bias-detection/code/network.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m words_ids \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrange(\u001b[39mlen\u001b[39m(vocab), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint64)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ruasgar/Bureau/text-bias-detection/code/network.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m vocab_init \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mlookup\u001b[39m.\u001b[39mKeyValueTensorInitializer(words,words_ids)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "train_df_copy = train[0]\n",
    "\n",
    "vocab = Counter()\n",
    "for sentence in train_df_copy['sentence']:  \n",
    "    vocab.update(sentence)\n",
    "\n",
    "words = tf.constant([word for word,_ in vocab])\n",
    "words_ids = tf.range(len(vocab), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words,words_ids)\n",
    "num_oov_buckets = 5000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init,num_oov_buckets)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convertendo os tokens para o índice na frequência (tanto da sentença, como das palavras que denotam viés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>news_link</th>\n",
       "      <th>outlet</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>group_id</th>\n",
       "      <th>num_sent</th>\n",
       "      <th>Label_bias</th>\n",
       "      <th>Label_opinion</th>\n",
       "      <th>article</th>\n",
       "      <th>biased_words4</th>\n",
       "      <th>full_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>(tf.Tensor(38, shape=(), dtype=int32), tf.Tens...</td>\n",
       "      <td>https://thefederalist.com/2019/08/13/need-chri...</td>\n",
       "      <td>federalist</td>\n",
       "      <td>white-nationalism</td>\n",
       "      <td>right</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>Biased</td>\n",
       "      <td>Somewhat factual but also opinionated</td>\n",
       "      <td>Our religious liberty never proceeded from att...</td>\n",
       "      <td>(tf.Tensor(0, shape=(), dtype=int32), tf.Tenso...</td>\n",
       "      <td>('The House Ways and Means Committee will hold...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence   \n",
       "867  (tf.Tensor(38, shape=(), dtype=int32), tf.Tens...  \\\n",
       "\n",
       "                                             news_link      outlet   \n",
       "867  https://thefederalist.com/2019/08/13/need-chri...  federalist  \\\n",
       "\n",
       "                 topic   type  group_id  num_sent Label_bias   \n",
       "867  white-nationalism  right        52         1     Biased  \\\n",
       "\n",
       "                             Label_opinion   \n",
       "867  Somewhat factual but also opinionated  \\\n",
       "\n",
       "                                               article   \n",
       "867  Our religious liberty never proceeded from att...  \\\n",
       "\n",
       "                                         biased_words4   \n",
       "867  (tf.Tensor(0, shape=(), dtype=int32), tf.Tenso...  \\\n",
       "\n",
       "                                          full_article  \n",
       "867  ('The House Ways and Means Committee will hold...  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_words_to_freq(data, vocab):\n",
    "\n",
    "    for index,row in data.iterrows():\n",
    "\n",
    "        sentence = row['sentence']\n",
    "        biased_words = row['biased_words4']\n",
    "\n",
    "        sentence = list(map(lambda word:int(vocab[word]),sentence))\n",
    "        biased_words = list(map(lambda word:int(vocab[word]),biased_words))\n",
    "\n",
    "        data.at[index,'sentence'] = tf.constant(sentence)\n",
    "        data.at[index, 'biased_words4'] = tf.constant(biased_words)\n",
    "\n",
    "    return data\n",
    "\n",
    "train_df_copy = convert_words_to_freq(train_df_copy, vocab)\n",
    "train_df_copy.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4597/557190795.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['sentence'] = tf.keras.preprocessing.sequence.pad_sequences(\n"
     ]
    }
   ],
   "source": [
    "def add_padding(data):\n",
    "\n",
    "    data['sentence'] = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                            data['sentence'],  \n",
    "                            value=[0]\n",
    "                        )\n",
    "    return data\n",
    "\n",
    "train_df_copy = add_padding(train_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>news_link</th>\n",
       "      <th>outlet</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>group_id</th>\n",
       "      <th>num_sent</th>\n",
       "      <th>Label_bias</th>\n",
       "      <th>Label_opinion</th>\n",
       "      <th>article</th>\n",
       "      <th>biased_words4</th>\n",
       "      <th>full_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>0</td>\n",
       "      <td>https://thefederalist.com/2019/08/13/need-chri...</td>\n",
       "      <td>federalist</td>\n",
       "      <td>white-nationalism</td>\n",
       "      <td>right</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>Biased</td>\n",
       "      <td>Somewhat factual but also opinionated</td>\n",
       "      <td>Our religious liberty never proceeded from att...</td>\n",
       "      <td>(tf.Tensor(0, shape=(), dtype=int32), tf.Tenso...</td>\n",
       "      <td>('The House Ways and Means Committee will hold...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.alternet.org/2019/09/everything-yo...</td>\n",
       "      <td>alternet</td>\n",
       "      <td>sport</td>\n",
       "      <td>left</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>Non-biased</td>\n",
       "      <td>Entirely factual</td>\n",
       "      <td>Since 1983, Kim Karsh has helped baseball team...</td>\n",
       "      <td>(tf.Tensor(0, shape=(), dtype=int32), tf.Tenso...</td>\n",
       "      <td>('There have been at least 555 confirmed cases...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reuters.com/article/climatechange-...</td>\n",
       "      <td>reuters</td>\n",
       "      <td>environment</td>\n",
       "      <td>center</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>Non-biased</td>\n",
       "      <td>Entirely factual</td>\n",
       "      <td>COPENHAGEN/OSLO, March 10 (Reuters) - The oil ...</td>\n",
       "      <td>(tf.Tensor(0, shape=(), dtype=int32), tf.Tenso...</td>\n",
       "      <td>('A recent study found that college students f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.alternet.org/2019/01/here-are-5-of...</td>\n",
       "      <td>alternet</td>\n",
       "      <td>immigration</td>\n",
       "      <td>left</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>Biased</td>\n",
       "      <td>Somewhat factual but also opinionated</td>\n",
       "      <td>Speaking to the country for the first time fro...</td>\n",
       "      <td>(tf.Tensor(0, shape=(), dtype=int32), tf.Tenso...</td>\n",
       "      <td>('President Obama has ordered the Department o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.foxnews.com/health/experts-warn-of...</td>\n",
       "      <td>fox-news</td>\n",
       "      <td>sport</td>\n",
       "      <td>right</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>Non-biased</td>\n",
       "      <td>Entirely factual</td>\n",
       "      <td>David Ayala was 11 years old when he began com...</td>\n",
       "      <td>(tf.Tensor(0, shape=(), dtype=int32), tf.Tenso...</td>\n",
       "      <td>('House Republicans are gearing up for a susta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence                                          news_link      outlet   \n",
       "867         0  https://thefederalist.com/2019/08/13/need-chri...  federalist  \\\n",
       "692         0  https://www.alternet.org/2019/09/everything-yo...    alternet   \n",
       "698         0  https://www.reuters.com/article/climatechange-...     reuters   \n",
       "416         0  https://www.alternet.org/2019/01/here-are-5-of...    alternet   \n",
       "346         0  https://www.foxnews.com/health/experts-warn-of...    fox-news   \n",
       "\n",
       "                 topic    type  group_id  num_sent  Label_bias   \n",
       "867  white-nationalism   right        52         1      Biased  \\\n",
       "692              sport    left        41         1  Non-biased   \n",
       "698        environment  center        41         1  Non-biased   \n",
       "416        immigration    left        25         1      Biased   \n",
       "346              sport   right        21         1  Non-biased   \n",
       "\n",
       "                             Label_opinion   \n",
       "867  Somewhat factual but also opinionated  \\\n",
       "692                       Entirely factual   \n",
       "698                       Entirely factual   \n",
       "416  Somewhat factual but also opinionated   \n",
       "346                       Entirely factual   \n",
       "\n",
       "                                               article   \n",
       "867  Our religious liberty never proceeded from att...  \\\n",
       "692  Since 1983, Kim Karsh has helped baseball team...   \n",
       "698  COPENHAGEN/OSLO, March 10 (Reuters) - The oil ...   \n",
       "416  Speaking to the country for the first time fro...   \n",
       "346  David Ayala was 11 years old when he began com...   \n",
       "\n",
       "                                         biased_words4   \n",
       "867  (tf.Tensor(0, shape=(), dtype=int32), tf.Tenso...  \\\n",
       "692  (tf.Tensor(0, shape=(), dtype=int32), tf.Tenso...   \n",
       "698  (tf.Tensor(0, shape=(), dtype=int32), tf.Tenso...   \n",
       "416  (tf.Tensor(0, shape=(), dtype=int32), tf.Tenso...   \n",
       "346  (tf.Tensor(0, shape=(), dtype=int32), tf.Tenso...   \n",
       "\n",
       "                                          full_article  \n",
       "867  ('The House Ways and Means Committee will hold...  \n",
       "692  ('There have been at least 555 confirmed cases...  \n",
       "698  ('A recent study found that college students f...  \n",
       "416  ('President Obama has ordered the Department o...  \n",
       "346  ('House Republicans are gearing up for a susta...  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_copy.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
