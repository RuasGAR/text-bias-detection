{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais\n",
    "\n",
    "Esse notebook tratará da implementação, do treino e da avaliação inicial de redes neurais recorrentes (especificamente de células LSTM) na busca pela solução do seguinte problema de classificação: identificação de de viés político em textos. O tamanho desses textos é algo a ser arbitrado e até mesmo utilizado como comparação: podemos utilizar somente as sentenças que foram marcadas por conter viés, ou mesmo o texto na íntegra da notícia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import ast\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit \n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Carregamento do dataset e do vocabulário, totalmente perfilados pelo notebook de pré-processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/final_dataset.csv')\n",
    "\n",
    "# transforma as listas que estão em string em listas novamente\n",
    "data[\"sentence\"] = data[\"sentence\"].apply(eval)\n",
    "data[\"article\"] = data[\"article\"].apply(eval)\n",
    "data[\"biased_words4\"] = data[\"biased_words4\"].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma as listas em Arrays do Numpy\n",
    "data[\"sentence\"] = data[\"sentence\"].apply(np.asarray)\n",
    "data[\"article\"] = data[\"article\"].apply(np.asarray)\n",
    "data[\"biased_words4\"] = data[\"biased_words4\"].apply(np.asarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence', 'outlet_alternet', 'outlet_breitbart', 'outlet_federalist',\n",
       "       'outlet_fox-news', 'outlet_huffpost', 'outlet_msnbc', 'outlet_reuters',\n",
       "       'outlet_usa-today', 'topic_abortion', 'topic_coronavirus',\n",
       "       'topic_elections-2020', 'topic_environment', 'topic_gender',\n",
       "       'topic_gun-control', 'topic_immigration',\n",
       "       'topic_international-politics-and-world-news', 'topic_middle-class',\n",
       "       'topic_sport', 'topic_student-debt', 'topic_trump-presidency',\n",
       "       'topic_vaccines', 'topic_white-nationalism', 'type_center', 'type_left',\n",
       "       'type_right', 'num_sent', 'Label_bias_Biased',\n",
       "       'Label_bias_No agreement', 'Label_bias_Non-biased', 'article',\n",
       "       'biased_words4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/vocabulary_table.pkl', 'rb') as f:\n",
    "    vocab_dict = pickle.load(f)\n",
    "\n",
    "keys = list(vocab_dict.keys())\n",
    "values = list(vocab_dict.values())\n",
    "n_oov = 5000\n",
    "\n",
    "vocab_table = tf.lookup.StaticVocabularyTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(keys, values, tf.string, tf.int64),\n",
    "    n_oov\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Separando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = ['Label_bias_Biased','Label_bias_No agreement', 'Label_bias_Non-biased']\n",
    "labels = data[label_columns]\n",
    "features = data.drop(label_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split(features, labels):\n",
    "    \"\"\" Retorna uma lista de tuplas contendo os datasets de features e de labels para cada segmento (treino, validação, teste) \"\"\"\n",
    "\n",
    "    # Treino-val e Teste\n",
    "    shuffle_train_test = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=892)\n",
    "    train_val_indexes, test_indexes = next(shuffle_train_test.split(features.values, labels.values))\n",
    "    train_val_df, train_val_labels = features.iloc[train_val_indexes], labels.iloc[train_val_indexes]\n",
    "    test_df, test_labels = features.iloc[test_indexes], labels.iloc[test_indexes]\n",
    "\n",
    "    # Treino e Validação\n",
    "    shuffle_train_validate = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=124)\n",
    "    train_indexes, validation_indexes = next(shuffle_train_validate.split(train_val_df.values,train_val_labels.values))\n",
    "    train_df, train_labels = features.iloc[train_indexes], labels.iloc[train_indexes]\n",
    "    validation_df, validation_labels = features.iloc[validation_indexes], labels.iloc[validation_indexes]\n",
    "\n",
    "\n",
    "    return [(train_df, train_labels), (validation_df, validation_labels), (test_df, test_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino:(1014, 29) ----- Validação: (338, 29) ------ Teste: (338, 29)\n"
     ]
    }
   ],
   "source": [
    "train, validation, test = train_valid_test_split(features,labels)\n",
    "\n",
    "train_df, train_labels = train\n",
    "validation_df, validation_labels = validation\n",
    "test_df, test_labels = test\n",
    "\n",
    "print(f\"Treino:{train_df.shape} ----- Validação: {validation_df.shape} ------ Teste: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Estruturando arquitetura da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "\n",
    "# parte textual\n",
    "text_input_length = len(train_df.loc[0,'article']) + len(train_df.loc[0,'biased_words4']) + len(train_df.loc[0,'sentence'])\n",
    "text_input = keras.layers.Input(shape=(text_input_length,))\n",
    "embedding_layer = keras.layers.Embedding(input_dim=int(vocab_table.size())+5000,\n",
    "                                         output_dim=embed_size,\n",
    "                                         mask_zero=True)(text_input)\n",
    "\n",
    "embedding_output = keras.layers.Bidirectional(keras.layers.LSTM(embed_size))(embedding_layer) \n",
    "\n",
    "\n",
    "# outras features \n",
    "feature_input_length = len(train_df.columns) - 3\n",
    "feature_input = keras.layers.Input(shape=(feature_input_length,))\n",
    "\n",
    "concatenated = keras.layers.concatenate([embedding_output,feature_input], axis=-1)\n",
    "reshaped = keras.layers.Reshape((-1, concatenated.shape[-1]))(concatenated)  # Add time dimension\n",
    "gru_layer1 = keras.layers.GRU(64)(reshaped)\n",
    "output = keras.layers.Dense(3, activation=\"softmax\")(gru_layer1)\n",
    "\n",
    "model = tf.keras.Model(inputs=[text_input,feature_input], outputs=output)\n",
    "#model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)        [(None, 3066)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)     (None, 3066, 128)            4588032   ['input_9[0][0]']             \n",
      "                                                                                                  \n",
      " bidirectional_4 (Bidirecti  (None, 256)                  263168    ['embedding_4[0][0]']         \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)       [(None, 26)]                 0         []                            \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate  (None, 282)                  0         ['bidirectional_4[0][0]',     \n",
      " )                                                                   'input_10[0][0]']            \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)         (None, 1, 282)               0         ['concatenate_4[0][0]']       \n",
      "                                                                                                  \n",
      " gru_4 (GRU)                 (None, 64)                   66816     ['reshape_4[0][0]']           \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 3)                    195       ['gru_4[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4918211 (18.76 MB)\n",
      "Trainable params: 4918211 (18.76 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = ['sentence','article','biased_words4']\n",
    "others = list(filter(lambda x: x if x not in text_cols else [],train_df.columns))\n",
    "\n",
    "train_df = pd.concat([train_df[text_cols], train_df[others]],axis=1)\n",
    "validation_df = pd.concat([validation_df[text_cols], validation_df[others]],axis=1)\n",
    "test_df = pd.concat([test_df[text_cols], test_df[others]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93      [1, 13361, 843, 5621, 172, 6203, 408, 566, 242...\n",
       "526     [1, 1434, 1039, 2681, 1397, 701, 281, 304, 136...\n",
       "761     [1, 1472, 80, 3802, 191, 701, 254, 4393, 11330...\n",
       "686     [1, 4711, 12583, 3519, 489, 14024, 2248, 485, ...\n",
       "1067    [1, 12645, 145, 154, 145, 2301, 1043, 63, 1643...\n",
       "                              ...                        \n",
       "1047    [1, 888, 1787, 3796, 966, 13877, 13878, 17365,...\n",
       "1327    [1, 9015, 1567, 3828, 1678, 17015, 644, 2395, ...\n",
       "1302    [1, 522, 1081, 458, 4056, 6374, 304, 3236, 162...\n",
       "245     [1, 864, 776, 421, 168, 265, 4140, 3470, 1775,...\n",
       "1000    [1, 248, 1114, 3765, 3761, 17010, 145, 513, 88...\n",
       "Name: sentence, Length: 1014, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalidation_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/bmt-env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/bmt-env/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "model.fit(train_df, train_labels, batch_size=32, epochs=10, validation_data=(validation_df, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
