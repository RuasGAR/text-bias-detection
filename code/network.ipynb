{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais e sua Implementação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ruasgar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/final_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence', 'news_link', 'outlet', 'topic', 'type', 'group_id',\n",
       "       'num_sent', 'Label_bias', 'Label_opinion', 'article', 'biased_words4',\n",
       "       'full_article'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Features Importantes\n",
    "- Label_bias (categórica): Indica se o texto foi classificado como enviesado, não-enviesado ou se não foi possível atingir um consenso quanto a classificação.\n",
    "- Label_opinion (categórica): Indica de que modo o viés se manifesta na percepção dos entrevistados; especificamente separando casos de exposição de opinião do autor ou com fatos que corroborem um viés. (Um pouco fuzzy demais, talvez?)\n",
    "- Biased_words(vetor): Indica as palavras marcadas como \"denunciantes\" da presença de viés. \n",
    "- Topic(categórica): Indica o assunto do texto, dentro das categorias PREENCHER AQUI   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Pré-Processamento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Básico textual (remoção de stopwords, filtro de expressões com números, etc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A priori, vamos considerar como tokens todas as palavras que:\n",
    "    - tenham mais do que 3 letras\n",
    "    - não possuam números\n",
    "    - não estejam nas stopwords do inglês\n",
    "    \n",
    "- Vamos remover os acentos e, mediante a escolha do usuário, aplicar um stemmer.\n",
    "- Visando o mapeamento das palavras para números inteiros, vamos utilizar o padrão de marcar alguns tokens especiais:\n",
    "    - 0: padding (completar o tamanho das representações)\n",
    "    - 1: Start-of-Sequence\n",
    "    - 2: tokens desconhecidos, necessários para novas palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [youtube, making, clear, birtherism, platform,...\n",
       "1    [increasingly, bitter, dispute, american, wome...\n",
       "2    [may, humanitarian, crisis, driving, vulnerabl...\n",
       "3    [professor, teaches, climate, change, classes,...\n",
       "4    [world, antidoping, agency, tuesday, said, rus...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_pattern = r'\\d+(\\.\\d+)?'\n",
    "solo_quotations_pattern = pattern = r\"^(?:' '|\\\\\" \"|\\`\\`)$\"\n",
    "\n",
    "remove_quotation = r\"'\\b|\\b'\\s|\\s'\\b|\\\"\\b|\\b\\\"\\s|\\s\\\"\\b|``\\b|\\b``\\s|\\s``\\b\" # como em \"America is Great\"\n",
    "remove_symbols_only = r'\\b[^\\w\\s]+\\b' # como em '--'\n",
    "remove_symbols_if_aside = r'\\b[^\\w\\s]|[^\\w\\s]\\b' # como em 'dog-'\n",
    "\n",
    "remove_patterns = remove_quotation,remove_symbols_if_aside, remove_symbols_only\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # talvez o set deixe mais rápido\n",
    "\n",
    "def preprocess_sentences(df):\n",
    "    \n",
    "    prepped_sentences = []\n",
    "\n",
    "    for s in df['sentence']:\n",
    "        # Remove acentos e põe tudo para lower case\n",
    "        s = unidecode(s).lower()\n",
    "        tokens = word_tokenize(s,language=\"english\")\n",
    "\n",
    "        tokens = [\n",
    "            re.sub('|'.join(remove_patterns),'',t)\n",
    "            for t in tokens \n",
    "            if not bool(re.search(digit_pattern,t))\n",
    "            and t not in string.punctuation\n",
    "            and t not in stop_words\n",
    "        ]\n",
    "\n",
    "        tokens = [\n",
    "            t for t in tokens\n",
    "            if not bool(re.match(solo_quotations_pattern,t)) \n",
    "            and len(t) >= 3\n",
    "        ]\n",
    "        \n",
    "        prepped_sentences.append(tokens)\n",
    "\n",
    "    df['sentence'] = prepped_sentences\n",
    "\n",
    "    return df \n",
    "\n",
    "df_functions_test_copy = df.copy()\n",
    "df_functions_test_copy = preprocess_sentences(df_functions_test_copy)\n",
    "df_functions_test_copy['sentence'].head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Stemmers\n",
    "\n",
    "- Temos o objetivo de avaliar como diferentes estratégias de stemming afetam os resultados do treinamento. A função a seguir visa abstrair esse passo para um etapa separada de pré-processamento. \n",
    "- Serão testados os stemmer de Porter e de Lancaster, além da lematização via WordNet. Vamos também obter os resultados do modelo quando utilizados sem qualquer stemmer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stemmer(sentence, stemmer_type:str):\n",
    "\n",
    "    # Função a ser executada para cada sentença, com expectativa de uso em algum filtro, por exemplo\n",
    "\n",
    "    stemmer = {}\n",
    "    lemmatizer = {}\n",
    "\n",
    "    if stemmer_type == \"Porter\":\n",
    "        stemmer = PorterStemmer()\n",
    "        return [stemmer.stem(token) for token in sentence]\n",
    "\n",
    "    elif stemmer_type == \"Lancaster\":\n",
    "        stemmer = LancasterStemmer()\n",
    "        return [stemmer.stem(token) for token in sentence]\n",
    "    \n",
    "    else:  # \"Wordnet\":\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return [lemmatizer.stem(token) for token in sentence]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Aplicando stemmer para teste da função "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [youtub, make, clear, birther, platform, year,...\n",
       "1    [increasingli, bitter, disput, american, women...\n",
       "2    [may, humanitarian, crisi, drive, vulner, peop...\n",
       "3    [professor, teach, climat, chang, class, subje...\n",
       "4    [world, antidop, agenc, tuesday, said, russian...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_functions_test_copy['sentence'] = df_functions_test_copy['sentence'].apply(lambda x: apply_stemmer(x,\"Porter\"))\n",
    "df_functions_test_copy['sentence'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Encoding do Vocabulário"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Separando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split(features, labels):\n",
    "    \"\"\" Retorna uma lista de tuplas contendo os datasets de features e de labels para cada segmento (treino, validação, teste) \"\"\"\n",
    "    \n",
    "    # Treino-val e Teste\n",
    "    shuffle_train_test = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=892)\n",
    "    train_val_indexes, test_indexes = next(shuffle_train_test.split(features.values, labels.values))\n",
    "    train_val_df, train_val_labels = features.iloc[train_val_indexes], labels.iloc[train_val_indexes]\n",
    "    test_df, test_labels = features.iloc[test_indexes], labels.iloc[test_indexes]\n",
    "\n",
    "    # Treino e Validação\n",
    "    shuffle_train_validate = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=124)\n",
    "    train_indexes, validation_indexes = next(shuffle_train_validate.split(train_val_df.values,train_val_labels.values))\n",
    "    train_df, train_labels = features.iloc[train_indexes], labels.iloc[train_indexes]\n",
    "    validation_df, validation_labels = features.iloc[validation_indexes], labels.iloc[validation_indexes]\n",
    "\n",
    "\n",
    "    return [(train_df, train_labels), (validation_df, validation_labels), (test_df, test_labels)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(848, 12)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, val, test = train_valid_test_split(df_functions_test_copy,df_functions_test_copy['type'])\n",
    "train[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
