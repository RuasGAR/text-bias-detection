{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvGwydOHnxrH"
      },
      "source": [
        "# Redes Neurais e sua Implementação"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKkxFUWTn6-5",
        "outputId": "6a3dd7f3-e9d3-4144-c86e-8d566f86fcd8"
      },
      "execution_count": 1310,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1311,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ySTH8jKnxrK",
        "outputId": "d3c57a6e-8504-45cd-cd07-c37b21aacb67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1311
        }
      ],
      "source": [
        "import string\n",
        "import re\n",
        "import nltk\n",
        "import ast\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from unidecode import unidecode\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1312,
      "metadata": {
        "id": "c1auSdJjnxrO"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"./data/final_dataset.csv\",converters={'biased_words4':ast.literal_eval})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1313,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU1iXOXxnxrO",
        "outputId": "4e11870c-7d4d-4dd9-cb56-f89eaeff12ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sentence', 'news_link', 'outlet', 'topic', 'type', 'group_id',\n",
              "       'num_sent', 'Label_bias', 'Label_opinion', 'article', 'biased_words4',\n",
              "       'full_article'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 1313
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i87WgYR5nxrP"
      },
      "source": [
        "## 1 - Features Importantes\n",
        "- Label_bias (categórica): Indica se o texto foi classificado como enviesado, não-enviesado ou se não foi possível atingir um consenso quanto a classificação.\n",
        "- Label_opinion (categórica): Indica de que modo o viés se manifesta na percepção dos entrevistados; especificamente separando casos de exposição de opinião do autor ou com fatos que corroborem um viés. (Um pouco fuzzy demais, talvez?)\n",
        "- Biased_words(vetor): Indica as palavras marcadas como \"denunciantes\" da presença de viés. \n",
        "- Topic(categórica): Indica o assunto do texto, dentro das categorias PREENCHER AQUI   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi7I8bprnxrP"
      },
      "source": [
        "## 2 - Pré-Processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jhbot7NnxrP"
      },
      "source": [
        "### 2.1 - Básico textual (remoção de stopwords, filtro de expressões com números, etc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIl3QssXnxrQ"
      },
      "source": [
        "- A priori, vamos considerar como tokens todas as palavras que:\n",
        "    - tenham mais do que 3 letras\n",
        "    - não possuam números\n",
        "    - não estejam nas stopwords do inglês\n",
        "    \n",
        "- Vamos remover os acentos e, mediante a escolha do usuário, aplicar um stemmer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1314,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufcufOOonxrQ",
        "outputId": "c55f5508-3df0-4508-82b9-531f7ee41336"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [youtube, making, clear, birtherism, platform,...\n",
              "1    [increasingly, bitter, dispute, american, wome...\n",
              "2    [may, humanitarian, crisis, driving, vulnerabl...\n",
              "3    [professor, teaches, climate, change, classes,...\n",
              "4    [world, antidoping, agency, tuesday, said, rus...\n",
              "Name: sentence, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 1314
        }
      ],
      "source": [
        "digit_pattern = r'\\d+(\\.\\d+)?'\n",
        "solo_quotations_pattern = pattern = r\"^(?:' '|\\\\\" \"|\\`\\`)$\"\n",
        "\n",
        "remove_quotation = r\"'\\b|\\b'\\s|\\s'\\b|\\\"\\b|\\b\\\"\\s|\\s\\\"\\b|``\\b|\\b``\\s|\\s``\\b\" # como em \"America is Great\"\n",
        "remove_symbols_only = r'\\b[^\\w\\s]+\\b' # como em '--'\n",
        "remove_symbols_if_aside = r'\\b[^\\w\\s]|[^\\w\\s]\\b' # como em 'dog-'\n",
        "\n",
        "remove_patterns = remove_quotation,remove_symbols_if_aside, remove_symbols_only\n",
        "\n",
        "stop_words = set(stopwords.words('english')) # talvez o set deixe mais rápido\n",
        "\n",
        "def preprocess_basic_text(data,col):\n",
        "    \n",
        "    prepped_texts = []\n",
        "\n",
        "    for s in data[col]:\n",
        "        \n",
        "        if type(s) == list:\n",
        "          s = ','.join(s)\n",
        "        # Remove acentos e põe tudo para lower case\n",
        "        s = unidecode(s).lower()\n",
        "        tokens = word_tokenize(s,language=\"english\")\n",
        "\n",
        "        tokens = [\n",
        "            re.sub('|'.join(remove_patterns),'',t)\n",
        "            for t in tokens \n",
        "            if not bool(re.search(digit_pattern,t))\n",
        "            and t not in string.punctuation\n",
        "            and t not in stop_words\n",
        "        ]\n",
        "\n",
        "        tokens = [\n",
        "            t for t in tokens\n",
        "            if not bool(re.match(solo_quotations_pattern,t)) \n",
        "            and len(t) >= 3\n",
        "        ]\n",
        "\n",
        "        prepped_texts.append(tokens)\n",
        "\n",
        "    data[col] = prepped_texts\n",
        "\n",
        "    return data\n",
        "\n",
        "df_copy = df.copy()\n",
        "df_copy = preprocess_basic_text(df_copy,'sentence')\n",
        "df_copy = preprocess_basic_text(df_copy,'biased_words4') # TAMBÉM COM AS PALAVRAS INDICATIVAS\n",
        "df_copy['sentence'].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dgVKAMqnxrR"
      },
      "source": [
        "### 2.2 - Stemmers\n",
        "\n",
        "- Temos o objetivo de avaliar como diferentes estratégias de stemming afetam os resultados do treinamento. A função a seguir visa abstrair esse passo para um etapa separada de pré-processamento. \n",
        "- Serão testados os stemmer de Porter e de Lancaster, além da lematização via WordNet. Vamos também obter os resultados do modelo quando utilizados sem qualquer stemmer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1315,
      "metadata": {
        "id": "4xM7KssbnxrR"
      },
      "outputs": [],
      "source": [
        "def apply_stemmer_individual(sentence, stemmer_type:str):\n",
        "\n",
        "    # Função a ser executada para cada sentença, com expectativa de uso em algum filtro, por exemplo\n",
        "\n",
        "    stemmer = {}\n",
        "    lemmatizer = {}\n",
        "\n",
        "    if stemmer_type == \"Porter\":\n",
        "        stemmer = PorterStemmer()\n",
        "        return [stemmer.stem(token) for token in sentence]\n",
        "\n",
        "    elif stemmer_type == \"Lancaster\":\n",
        "        stemmer = LancasterStemmer()\n",
        "        return [stemmer.stem(token) for token in sentence]\n",
        "    \n",
        "    else:  # \"Wordnet\":\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        return [lemmatizer.stem(token) for token in sentence]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn5oI00HnxrR"
      },
      "source": [
        "- Aplicando stemmer para teste da função "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1316,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgjPdj9rnxrS",
        "outputId": "d58d4066-4b90-4172-a3b0-53e425fedb2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [youtub, make, clear, birther, platform, year,...\n",
              "1    [increasingli, bitter, disput, american, women...\n",
              "2    [may, humanitarian, crisi, drive, vulner, peop...\n",
              "3    [professor, teach, climat, chang, class, subje...\n",
              "4    [world, antidop, agenc, tuesday, said, russian...\n",
              "Name: sentence, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 1316
        }
      ],
      "source": [
        "df_copy['sentence'] = df_copy['sentence'].apply(lambda x: apply_stemmer_individual(x,\"Porter\"))\n",
        "df_copy['sentence'].head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generalizando a aplicação pra todas as instâncias\n",
        "def apply_stemmer_for_all(data,cols,stemmer_flag=False,stemmer_type=None):\n",
        "  \n",
        "  if stemmer_flag == False:\n",
        "    return data\n",
        "\n",
        "  for col in cols:\n",
        "    data[col] = data[col].apply(lambda x: apply_stemmer_individual(x,stemmer_type))\n",
        "  return data"
      ],
      "metadata": {
        "id": "glvQTh33ftLq"
      },
      "execution_count": 1317,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Aplicando stemmer nas palavras enviesadas também"
      ],
      "metadata": {
        "id": "q7DI_NtDL3Wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy['biased_words4'] = df_copy['biased_words4'].apply(lambda x: apply_stemmer_individual(x,\"Porter\"))\n",
        "df_copy['biased_words4'][:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2poUveOL9Qy",
        "outputId": "3d521032-4e8a-486c-d455-47e8f79d4aee"
      },
      "execution_count": 1318,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [belat, birther]\n",
              "1            [bitter]\n",
              "2             [crisi]\n",
              "3           [legitim]\n",
              "4                  []\n",
              "Name: biased_words4, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 1318
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOOeFWainxrT"
      },
      "source": [
        "### 2.3 - Encoding (palavras)\n",
        "\n",
        "- Para representar as palavras, vamos criar um encoding com base na frequência apresentada no corpus.\n",
        "- Utilizaremos alguns números para representar determinadas semânticas:\n",
        "  - padding(\"pad\"), necessário para deixar todas as entradas com mesmo tamanho, terá o valor 0.\n",
        "  - start-of-sequence(\"sos\"), usado para identificar o ínicio de cada sentença, terá o número 1\n",
        "  - unknown, para marcar palavras desconhecidas, terá o número 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOS = \"<sos>\"\n",
        "PADDING = \"<pad>\"\n",
        "UNKNOWN = \"<ukn>\""
      ],
      "metadata": {
        "id": "p2aNLT_LEAaX"
      },
      "execution_count": 1319,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_padding_and_sos(data):\n",
        "\n",
        "    max_length = max(len(seq) for seq in data['sentence'])\n",
        "    for index,row in data.iterrows():\n",
        "      sentence = [SOS] + row['sentence']\n",
        "      paddings = (max_length - len(row['sentence'])) * [PADDING]\n",
        "      sentence += paddings\n",
        "      data.at[index,'sentence'] = sentence\n",
        "    \n",
        "    return data\n",
        "\n",
        "df_copy = add_padding_and_sos(df_copy)"
      ],
      "metadata": {
        "id": "IzA2U6fi3NYb"
      },
      "execution_count": 1320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_copy.loc[190,'sentence'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwR2dRIFHVq9",
        "outputId": "97bb0591-b11e-4298-fe83-804cdb7e29ea"
      },
      "execution_count": 1321,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<sos>', 'even', 'though', 'thunberg', 'capabl', 'thing', 'mani', 'right', 'refus', 'read', 'understand', 'scientif', 'evid', 'rightwing', 'argu', 'thunberg', 'fellow', 'youth', 'activist', 'think', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Construindo o vocabulário com base na frequência de aparecimento.\n",
        "  - *Obs*: note a colocação de determinadas tags logo no ínicio"
      ],
      "metadata": {
        "id": "212Rr3J3Ka06"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1322,
      "metadata": {
        "id": "-qkd_GWhnxrT"
      },
      "outputs": [],
      "source": [
        "def create_vocabulary(data):\n",
        "  \"\"\" Retorna uma lookup table (StaticVocabularyTable) \"\"\"  \n",
        "\n",
        "  vocab = Counter()\n",
        "  for sentence in data['sentence']:  \n",
        "      vocab.update(sentence)\n",
        "\n",
        "  words = tf.constant([PADDING] + \n",
        "                      [SOS] + \n",
        "                      [UNKNOWN] + \n",
        "                      [word for word in vocab.keys() if word != PADDING and word != SOS]\n",
        "  )\n",
        "  words_ids = tf.range(len(vocab)+1, dtype=tf.int64) # +1 porque unknown é o único não mapeado\n",
        "  vocab_init = tf.lookup.KeyValueTensorInitializer(words,words_ids)\n",
        "  num_oov_buckets = 5000 # bucket para palavras desconhecidas\n",
        "  vocab_table = tf.lookup.StaticVocabularyTable(vocab_init,num_oov_buckets)\n",
        "  \n",
        "  return vocab_table"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Demonstração de como estão os encodings de algumas palavras. "
      ],
      "metadata": {
        "id": "9aSx7Sx-Ph93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_table = create_vocabulary(df_copy)\n",
        "vocab_table.lookup(tf.constant([b'nice trump move'.split(b' ')]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ALwPlNXIq83",
        "outputId": "82ca6c62-d0a7-4543-808a-35572308bcbb"
      },
      "execution_count": 1323,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 3), dtype=int64, numpy=array([[2390,   97,  558]])>"
            ]
          },
          "metadata": {},
          "execution_count": 1323
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcuvVw23nxrT"
      },
      "source": [
        "- Convertendo os tokens para o índice na tabela (tanto da sentença, como das palavras que denotam viés)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1324,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "yS398UOqnxrT",
        "outputId": "94d2e9ed-1ebc-4fd2-a248-9f6e3ea104fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence  \\\n",
              "0  [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...   \n",
              "1  [1, 18, 19, 20, 21, 22, 23, 24, 25, 24, 26, 27...   \n",
              "\n",
              "                                           news_link     outlet  \\\n",
              "0  https://eu.usatoday.com/story/tech/2020/02/03/...  usa-today   \n",
              "1  https://www.nbcnews.com/news/sports/women-s-te...      msnbc   \n",
              "\n",
              "            topic    type  group_id  num_sent  Label_bias  \\\n",
              "0  elections-2020  center         1         1      Biased   \n",
              "1           sport    left         1         1  Non-biased   \n",
              "\n",
              "                           Label_opinion  \\\n",
              "0  Somewhat factual but also opinionated   \n",
              "1                       Entirely factual   \n",
              "\n",
              "                                             article biased_words4  \\\n",
              "0  YouTube says no ‘deepfakes’ or ‘birther’ video...       [11, 6]   \n",
              "1  FRISCO, Texas — The increasingly bitter disput...          [19]   \n",
              "\n",
              "                                        full_article  \n",
              "0  ('YouTube is making clear there will be no “bi...  \n",
              "1  ('', 'Profile', 'Sections', 'tv', 'Featured', ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0f368253-0a44-4993-8d08-6287c6c9aaf7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>news_link</th>\n",
              "      <th>outlet</th>\n",
              "      <th>topic</th>\n",
              "      <th>type</th>\n",
              "      <th>group_id</th>\n",
              "      <th>num_sent</th>\n",
              "      <th>Label_bias</th>\n",
              "      <th>Label_opinion</th>\n",
              "      <th>article</th>\n",
              "      <th>biased_words4</th>\n",
              "      <th>full_article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...</td>\n",
              "      <td>https://eu.usatoday.com/story/tech/2020/02/03/...</td>\n",
              "      <td>usa-today</td>\n",
              "      <td>elections-2020</td>\n",
              "      <td>center</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Biased</td>\n",
              "      <td>Somewhat factual but also opinionated</td>\n",
              "      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
              "      <td>[11, 6]</td>\n",
              "      <td>('YouTube is making clear there will be no “bi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1, 18, 19, 20, 21, 22, 23, 24, 25, 24, 26, 27...</td>\n",
              "      <td>https://www.nbcnews.com/news/sports/women-s-te...</td>\n",
              "      <td>msnbc</td>\n",
              "      <td>sport</td>\n",
              "      <td>left</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Non-biased</td>\n",
              "      <td>Entirely factual</td>\n",
              "      <td>FRISCO, Texas — The increasingly bitter disput...</td>\n",
              "      <td>[19]</td>\n",
              "      <td>('', 'Profile', 'Sections', 'tv', 'Featured', ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f368253-0a44-4993-8d08-6287c6c9aaf7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0f368253-0a44-4993-8d08-6287c6c9aaf7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0f368253-0a44-4993-8d08-6287c6c9aaf7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 1324
        }
      ],
      "source": [
        "def convert_words_to_freq(data, vocab):\n",
        "\n",
        "  for index, row in data.iterrows():\n",
        "    \n",
        "    sentence = tf.constant(row['sentence'])\n",
        "    biased_words = tf.constant(row['biased_words4'])\n",
        "\n",
        "    sentence_ided = vocab_table.lookup(sentence).numpy()\n",
        "    \n",
        "    biased_words_ided = vocab_table.lookup(biased_words).numpy() if len(biased_words) > 0 else []\n",
        "    \n",
        "    data.at[index, 'sentence'] = sentence_ided\n",
        "    data.at[index, 'biased_words4'] = biased_words_ided\n",
        "\n",
        "\n",
        "  return data\n",
        "\n",
        "df_copy = convert_words_to_freq(df_copy, vocab_table)\n",
        "df_copy.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 - One-hot encoding para features categóricas"
      ],
      "metadata": {
        "id": "28mGJnjWQC82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cats_to_one_hot(data,column_names):\n",
        "  for col in column_names:\n",
        "    df_encoded = pd.get_dummies(df[col], prefix=col)\n",
        "    cat_col_index = data.columns.get_loc(col)\n",
        "    data = pd.concat([data.iloc[:,:cat_col_index], df_encoded, data.iloc[:,cat_col_index+1:]],axis=1)\n",
        "    \n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "ItB9GETSQB7v"
      },
      "execution_count": 1325,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy = cats_to_one_hot(df_copy, column_names=['topic','outlet','type','Label_bias','Label_opinion'])\n",
        "df_copy.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "hQ7UUyZpSsUs",
        "outputId": "488f769a-c647-40d5-828b-a111a3c0d515"
      },
      "execution_count": 1326,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence  \\\n",
              "0  [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...   \n",
              "1  [1, 18, 19, 20, 21, 22, 23, 24, 25, 24, 26, 27...   \n",
              "2  [1, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51...   \n",
              "\n",
              "                                           news_link  outlet_alternet  \\\n",
              "0  https://eu.usatoday.com/story/tech/2020/02/03/...                0   \n",
              "1  https://www.nbcnews.com/news/sports/women-s-te...                0   \n",
              "2  https://www.alternet.org/2019/01/here-are-5-of...                1   \n",
              "\n",
              "   outlet_breitbart  outlet_federalist  outlet_fox-news  outlet_huffpost  \\\n",
              "0                 0                  0                0                0   \n",
              "1                 0                  0                0                0   \n",
              "2                 0                  0                0                0   \n",
              "\n",
              "   outlet_msnbc  outlet_reuters  outlet_usa-today  ...  Label_bias_Biased  \\\n",
              "0             0               0                 1  ...                  1   \n",
              "1             1               0                 0  ...                  0   \n",
              "2             0               0                 0  ...                  1   \n",
              "\n",
              "   Label_bias_No agreement  Label_bias_Non-biased  \\\n",
              "0                        0                      0   \n",
              "1                        0                      1   \n",
              "2                        0                      0   \n",
              "\n",
              "   Label_opinion_Entirely factual  Label_opinion_Expresses writer’s opinion  \\\n",
              "0                               0                                         0   \n",
              "1                               1                                         0   \n",
              "2                               0                                         1   \n",
              "\n",
              "   Label_opinion_No agreement  \\\n",
              "0                           0   \n",
              "1                           0   \n",
              "2                           0   \n",
              "\n",
              "   Label_opinion_Somewhat factual but also opinionated  \\\n",
              "0                                                  1     \n",
              "1                                                  0     \n",
              "2                                                  0     \n",
              "\n",
              "                                             article  biased_words4  \\\n",
              "0  YouTube says no ‘deepfakes’ or ‘birther’ video...        [11, 6]   \n",
              "1  FRISCO, Texas — The increasingly bitter disput...           [19]   \n",
              "2  Speaking to the country for the first time fro...           [43]   \n",
              "\n",
              "                                        full_article  \n",
              "0  ('YouTube is making clear there will be no “bi...  \n",
              "1  ('', 'Profile', 'Sections', 'tv', 'Featured', ...  \n",
              "2  ('Speaking to the country for the first time f...  \n",
              "\n",
              "[3 rows x 39 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-50ef2009-3069-4736-8ac9-abc23c38e35e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>news_link</th>\n",
              "      <th>outlet_alternet</th>\n",
              "      <th>outlet_breitbart</th>\n",
              "      <th>outlet_federalist</th>\n",
              "      <th>outlet_fox-news</th>\n",
              "      <th>outlet_huffpost</th>\n",
              "      <th>outlet_msnbc</th>\n",
              "      <th>outlet_reuters</th>\n",
              "      <th>outlet_usa-today</th>\n",
              "      <th>...</th>\n",
              "      <th>Label_bias_Biased</th>\n",
              "      <th>Label_bias_No agreement</th>\n",
              "      <th>Label_bias_Non-biased</th>\n",
              "      <th>Label_opinion_Entirely factual</th>\n",
              "      <th>Label_opinion_Expresses writer’s opinion</th>\n",
              "      <th>Label_opinion_No agreement</th>\n",
              "      <th>Label_opinion_Somewhat factual but also opinionated</th>\n",
              "      <th>article</th>\n",
              "      <th>biased_words4</th>\n",
              "      <th>full_article</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...</td>\n",
              "      <td>https://eu.usatoday.com/story/tech/2020/02/03/...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
              "      <td>[11, 6]</td>\n",
              "      <td>('YouTube is making clear there will be no “bi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1, 18, 19, 20, 21, 22, 23, 24, 25, 24, 26, 27...</td>\n",
              "      <td>https://www.nbcnews.com/news/sports/women-s-te...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>FRISCO, Texas — The increasingly bitter disput...</td>\n",
              "      <td>[19]</td>\n",
              "      <td>('', 'Profile', 'Sections', 'tv', 'Featured', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51...</td>\n",
              "      <td>https://www.alternet.org/2019/01/here-are-5-of...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Speaking to the country for the first time fro...</td>\n",
              "      <td>[43]</td>\n",
              "      <td>('Speaking to the country for the first time f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 39 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50ef2009-3069-4736-8ac9-abc23c38e35e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-50ef2009-3069-4736-8ac9-abc23c38e35e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-50ef2009-3069-4736-8ac9-abc23c38e35e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 1326
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 - Limpando features"
      ],
      "metadata": {
        "id": "hu0eDi-4UE47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_columns(data, column_names):\n",
        "  return data.drop(column_names,axis=1)"
      ],
      "metadata": {
        "id": "nufVpqv2Udwn"
      },
      "execution_count": 1327,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy = drop_columns(df_copy,column_names=['news_link','group_id','article','full_article'])\n",
        "df_copy.head(1)\n",
        "df_copy.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmFBOZxZXBAJ",
        "outputId": "7b50ce1b-8b86-4cb1-de4c-75adcc5e604d"
      },
      "execution_count": 1328,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1414, 35)"
            ]
          },
          "metadata": {},
          "execution_count": 1328
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6 - Compilando pipeline de pré-processamento"
      ],
      "metadata": {
        "id": "PSOuFsrwcT3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VocabularyConversionTransformer(BaseEstimator,TransformerMixin):\n",
        "  def __init__(self):\n",
        "    self.vocab_table = None\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    self.vocab_table = create_vocabulary(X)\n",
        "    return self\n",
        "  \n",
        "  def transform(self,X,y=None):\n",
        "    return convert_words_to_freq(X,vocab=self.vocab_table)"
      ],
      "metadata": {
        "id": "pED3BesynKbf"
      },
      "execution_count": 1329,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data,stemmer_flag=False,stemmer_type=None):\n",
        "  \n",
        "  one_hot_columns = ['topic','outlet','type','Label_bias','Label_opinion']\n",
        "  columns_to_drop = ['news_link','group_id','article','full_article']\n",
        "\n",
        "  preprocess_pipeline = ([\n",
        "      ('Tokenização (sentenças)', FunctionTransformer(preprocess_basic_text,kw_args={'col':'sentence'})),\n",
        "      ('Tokenização (biased_words)', FunctionTransformer(preprocess_basic_text,kw_args={'col':'biased_words4'})),\n",
        "      ('Stemming', FunctionTransformer(apply_stemmer_for_all, kw_args={'cols':['sentence','biased_words4'],\n",
        "                                                                      'stemmer_flag': stemmer_flag, \n",
        "                                                                      'stemmer_type': stemmer_type\n",
        "                                                                      })),\n",
        "      ('Adding SOS and Padding', FunctionTransformer(add_padding_and_sos)),\n",
        "      ('Converting words to its integer rep', VocabularyConversionTransformer()),\n",
        "      ('One-hot-encoding', FunctionTransformer(cats_to_one_hot,kw_args={'column_names':one_hot_columns})),\n",
        "      ('Dropping features', FunctionTransformer(drop_columns,kw_args={'column_names':columns_to_drop})),\n",
        "  ])\n",
        "\n",
        "  pipeline = Pipeline(preprocess_pipeline)\n",
        "\n",
        "  return pipeline.fit_transform(data)"
      ],
      "metadata": {
        "id": "FX_bDiJUcF91"
      },
      "execution_count": 1330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = preprocess(df.copy(),True,stemmer_type=\"Porter\")"
      ],
      "metadata": {
        "id": "PBRClwvPcTB0"
      },
      "execution_count": 1331,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9c9t39p9YWs",
        "outputId": "701d75fb-42c4-4e52-f479-a5233a3884d9"
      },
      "execution_count": 1332,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1414, 35)"
            ]
          },
          "metadata": {},
          "execution_count": 1332
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7 - Salvando dataset final\n"
      ],
      "metadata": {
        "id": "rKHlDGUvKR9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('../data/real_final_dataset.csv')"
      ],
      "metadata": {
        "id": "EEWE1g73KaVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTmunv1UnxrS"
      },
      "source": [
        "## 2.8 - Separando o dataset (SEPARAR ANTES DO PRÉ PROCESSAMENTO PARA PODER USAR A LABEL_BIAS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1333,
      "metadata": {
        "id": "47Q6I9gVnxrS"
      },
      "outputs": [],
      "source": [
        "def train_valid_test_split(features, labels):\n",
        "    \"\"\" Retorna uma lista de tuplas contendo os datasets de features e de labels para cada segmento (treino, validação, teste) \"\"\"\n",
        "    \n",
        "    # Treino-val e Teste\n",
        "    shuffle_train_test = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=892)\n",
        "    train_val_indexes, test_indexes = next(shuffle_train_test.split(features.values, labels.values))\n",
        "    train_val_df, train_val_labels = features.iloc[train_val_indexes], labels.iloc[train_val_indexes]\n",
        "    test_df, test_labels = features.iloc[test_indexes], labels.iloc[test_indexes]\n",
        "\n",
        "    # Treino e Validação\n",
        "    shuffle_train_validate = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=124)\n",
        "    train_indexes, validation_indexes = next(shuffle_train_validate.split(train_val_df.values,train_val_labels.values))\n",
        "    train_df, train_labels = features.iloc[train_indexes], labels.iloc[train_indexes]\n",
        "    validation_df, validation_labels = features.iloc[validation_indexes], labels.iloc[validation_indexes]\n",
        "\n",
        "\n",
        "    return [(train_df, train_labels), (validation_df, validation_labels), (test_df, test_labels)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1334,
      "metadata": {
        "id": "DwBxzoVAnxrS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "77200e3a-c856-4234-949e-16013f69c17f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Label_bias'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1334-307546a28973>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_valid_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_copy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_copy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label_bias'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Label_bias'"
          ]
        }
      ],
      "source": [
        "train, val, test = train_valid_test_split(df_copy,df_copy['Label_bias'])\n",
        "train[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 - Arquitetura da Rede Neural"
      ],
      "metadata": {
        "id": "e1hOXog6_sya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "embed_size = 128\n",
        "\n",
        "# parte textual\n",
        "text_input_length = len(data['sentence'][1]) \n",
        "text_based_inputs = tf.keras.layers.Embedding(int(vocab_table.size())+5000, embed_size,mask_zero=True, input_length=text_input_length)\n",
        "text_branch = tf.keras.Sequential()\n",
        "text_branch.add(text_based_inputs)\n",
        "text_branch.add(tf.keras.layers.Flatten())  \n",
        "\n",
        "# parte densa (features normais)\n",
        "feature_input_length = len(data.columns) - 2\n",
        "feature_input_layer = tf.keras.layers.Dense(10,input_shape=(None,feature_input_length))\n",
        "feature_branch = tf.keras.Sequential().add(feature_input_layer)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.concatenate([text_branch,feature_branch],axis=-1),\n",
        "    tf.keras.layers.Dense(1, activation='softmax'),\n",
        "    tf.keras.layers.GRU(128,return_sequences=True),\n",
        "    tf.keras.layers.GRU(128),\n",
        "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "fYoL4j7C_xPJ",
        "outputId": "0cfae6d3-6d92-45b3-aa41-ab5712954eed"
      },
      "execution_count": 1358,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1358-0849c50e5ec7>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m model = tf.keras.Sequential([\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_branch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_branch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/layers/merging/concatenate.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(inputs, axis, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mconcatenation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minputs\u001b[0m \u001b[0malongside\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \"\"\"\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/layers/merging/concatenate.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Used purely for shape validation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             raise ValueError(\n\u001b[1;32m     99\u001b[0m                 \u001b[0;34m\"A `Concatenate` layer should be called on a list of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8jhbot7NnxrP",
        "5dgVKAMqnxrR",
        "KOOeFWainxrT",
        "28mGJnjWQC82",
        "hu0eDi-4UE47"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}