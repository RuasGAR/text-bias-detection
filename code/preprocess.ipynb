{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rvGwydOHnxrH"
   },
   "source": [
    "# Pré-Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iKkxFUWTn6-5",
    "outputId": "61bd2040-e223-4d01-c007-53502d11169a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ruasgar/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.23ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/ruasgar/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.13.1-unknown is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "Requirement already satisfied: unidecode in /home/ruasgar/.local/lib/python3.8/site-packages (1.3.6)\n",
      "/home/ruasgar/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.23ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/ruasgar/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.13.1-unknown is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/ruasgar/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.23ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/ruasgar/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.13.1-unknown is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.5 MB 554 kB/s eta 0:00:01    |███                             | 2.4 MB 969 kB/s eta 0:00:25     |█████▌                          | 4.6 MB 504 kB/s eta 0:00:44     |███████▎                        | 6.0 MB 480 kB/s eta 0:00:43     |███████▋                        | 6.3 MB 480 kB/s eta 0:00:42     |█████████████████████████▍      | 21.0 MB 502 kB/s eta 0:00:11\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /home/ruasgar/.local/lib/python3.8/site-packages (from gensim) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/ruasgar/.local/lib/python3.8/site-packages (from gensim) (1.9.2)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 807 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.1 smart-open-6.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ySTH8jKnxrK",
    "outputId": "0e6b33a3-5b52-4aff-f79b-e5523c8064ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 01:48:04.796018: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-26 01:48:05.275649: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-26 01:48:05.276924: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-26 01:48:09.023416: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ruasgar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ruasgar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import ast\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from unidecode import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c1auSdJjnxrO"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/full_articles_dataset.csv\",converters={'biased_words4':ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wU1iXOXxnxrO",
    "outputId": "72ecd4ca-96c4-44bf-ccef-1358041cf1cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence', 'news_link', 'outlet', 'topic', 'type', 'group_id',\n",
       "       'num_sent', 'Label_bias', 'Label_opinion', 'article', 'biased_words4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "i87WgYR5nxrP"
   },
   "source": [
    "## 1 - Features Importantes\n",
    "- Label_bias (categórica): Indica se o texto foi classificado como enviesado, não-enviesado ou se não foi possível atingir um consenso quanto a classificação.\n",
    "- Biased_words(vetor): Indica as palavras marcadas como \"denunciantes\" da presença de viés.\n",
    "- Topic(categórica): Indica o assunto do texto, dentro das categorias\n",
    "- Type: Denota o posicionamento do viés, separados como esquerda, centro ou direita.\n",
    "- Outlet: Portal de origem da notícia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Biased', 'Non-biased', 'No agreement'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label_bias'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['elections-2020', 'sport', 'immigration', 'environment',\n",
       "       'abortion', 'student-debt', 'vaccines', 'white-nationalism',\n",
       "       'coronavirus', 'trump-presidency', 'middle-class', 'gender',\n",
       "       'international-politics-and-world-news', 'gun-control'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['topic'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['center', 'left', 'right'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['type'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Bi7I8bprnxrP"
   },
   "source": [
    "## 2 - Pré-Processamento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observação*: as funções relativas a entradas textuais são testadas individualmente com a coluna 'sentence'. Essa feature traz a frase na qual os participantes da pesquisa viram um viés.\n",
    "\n",
    "Quando formos realizar os treinos e a validação, utilizaremos todo o texto que compõe o documento analisado. Se mostrar-se interessante, podemos averiguar a performance reduzindo o corpus a, justamente, essas sentenças. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8jhbot7NnxrP"
   },
   "source": [
    "### 2.1 - Básico textual (remoção de stopwords, filtro de expressões com números, etc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIl3QssXnxrQ"
   },
   "source": [
    "- A priori, vamos considerar como tokens todas as palavras que:\n",
    "    - tenham mais do que 3 letras;\n",
    "    - não possuam números;\n",
    "    - não sejam siglas (como U.S.) nem que tenham um símbolo diferente de letra pós ou precedido por uma (como é o caso de separação entre palavras ao pular linha, e.g. \"fari-\");\n",
    "    - não estejam nas stopwords do inglês.\n",
    "\n",
    "- Uma decisão importante, especialmente no contexto de viés, é exclusão das das aspas. Isso pode ocorrer tanto de forma a ironizar acontecimentos como a corroborar algum conhecimento com argumento de autoridade enviesado, mas é algo que acreditamos estar diretamente relacionado ao conteúdo da frase, independetemente da grafia ao redor. Afinal, queremos tentar identificar a intenção. Seria interessante, para um próximo trabalho, tentar considerar as aspas e o que estivesse sob seus limites como algum tipo de incentivo ou demérito.\n",
    "\n",
    "- Mediante a escolha do usuário, aplicaremos um stemmer(Porter/Lancaster) ou um lemmatizer. Também podemos simplesmente não aplicar nenhum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufcufOOonxrQ",
    "outputId": "7394f544-8fc2-4b52-9847-40a35b98f3e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [youtube, making, clear, birtherism, platform,...\n",
       "1    [increasingly, bitter, dispute, american, wome...\n",
       "2    [may, humanitarian, crisis, driving, vulnerabl...\n",
       "3    [professor, teaches, climate, change, classes,...\n",
       "4    [looking, around, united, states, never, enoug...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit_pattern = r'\\d+(\\.\\d+)?'\n",
    "solo_quotations_pattern = pattern = r\"^(?:' '|\\\\\" \"|\\`\\`)$\"\n",
    "\n",
    "remove_quotation = r\"'\\b|\\b'\\s|\\s'\\b|\\\"\\b|\\b\\\"\\s|\\s\\\"\\b|``\\b|\\b``\\s|\\s``\\b\" # como em \"America is Great\"\n",
    "remove_symbols_only = r'\\b[^\\w\\s]+\\b' # como em '--'\n",
    "remove_symbols_if_aside = r'\\b[^\\w\\s]|[^\\w\\s]\\b' # como em 'dog-'\n",
    "\n",
    "remove_patterns = remove_quotation,remove_symbols_if_aside, remove_symbols_only\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # talvez o set deixe mais rápido\n",
    "\n",
    "def preprocess_basic_text(data,columns):\n",
    "\n",
    "    for col in columns:\n",
    "\n",
    "       prepped_texts = []\n",
    "\n",
    "       for doc in data[col]:\n",
    "\n",
    "            if type(doc) == list:\n",
    "              doc = ','.join(doc)\n",
    "            # Remove acentos e põe tudo para lower case\n",
    "            doc = unidecode(doc).lower()\n",
    "            tokens = word_tokenize(doc,language=\"english\")\n",
    "\n",
    "            tokens = [\n",
    "                re.sub('|'.join(remove_patterns),'',t)\n",
    "                for t in tokens\n",
    "                if not bool(re.search(digit_pattern,t))\n",
    "                and t not in string.punctuation\n",
    "                and t not in stop_words\n",
    "            ]\n",
    "\n",
    "            tokens = [\n",
    "                t for t in tokens\n",
    "                if not bool(re.match(solo_quotations_pattern,t))\n",
    "                and len(t) >= 3\n",
    "             ]\n",
    "\n",
    "            prepped_texts.append(tokens)\n",
    "\n",
    "       data[col] = prepped_texts\n",
    "\n",
    "    return data\n",
    "\n",
    "df_copy = df.copy()\n",
    "df_copy = preprocess_basic_text(df_copy,['sentence'])\n",
    "df_copy['sentence'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5dgVKAMqnxrR"
   },
   "source": [
    "### 2.2 - Stemmers\n",
    "\n",
    "- Temos o objetivo de avaliar como diferentes estratégias de stemming afetam os resultados do treinamento. A função a seguir visa abstrair esse passo para um etapa separada de pré-processamento.\n",
    "- Serão testados os stemmer de Porter e de Lancaster, além da lematização via WordNet. Vamos também obter os resultados do modelo quando utilizados sem qualquer stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4xM7KssbnxrR"
   },
   "outputs": [],
   "source": [
    "def apply_stemmer_individual(sentence, stemmer_type:str):\n",
    "\n",
    "    # Função a ser executada para cada sentença, com expectativa de uso em algum filtro, por exemplo\n",
    "\n",
    "    stemmer = {}\n",
    "    lemmatizer = {}\n",
    "\n",
    "    if stemmer_type == \"Porter\":\n",
    "        stemmer = PorterStemmer()\n",
    "        return [stemmer.stem(token) for token in sentence]\n",
    "\n",
    "    elif stemmer_type == \"Lancaster\":\n",
    "        stemmer = LancasterStemmer()\n",
    "        return [stemmer.stem(token) for token in sentence]\n",
    "\n",
    "    else:  # \"Wordnet\":\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return [lemmatizer.stem(token) for token in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KgjPdj9rnxrS",
    "outputId": "6e7bdbdd-6318-4b89-a7f9-47723c6ebe3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [youtub, make, clear, birther, platform, year,...\n",
       "1    [increasingli, bitter, disput, american, women...\n",
       "2    [may, humanitarian, crisi, drive, vulner, peop...\n",
       "3    [professor, teach, climat, chang, class, subje...\n",
       "4    [look, around, unit, state, never, enough, wel...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy['sentence'] = df_copy['sentence'].apply(lambda x: apply_stemmer_individual(x,\"Porter\"))\n",
    "df_copy['sentence'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "glvQTh33ftLq"
   },
   "outputs": [],
   "source": [
    "# generalizando a aplicação pra todas as instâncias\n",
    "def apply_stemmer_for_all(data,columns,stemmer_flag=False,stemmer_type=None):\n",
    "\n",
    "  if stemmer_flag == False:\n",
    "    return data\n",
    "\n",
    "  for col in columns:\n",
    "    data[col] = data[col].apply(lambda x: apply_stemmer_individual(x,stemmer_type))\n",
    "  return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KOOeFWainxrT"
   },
   "source": [
    "### 2.3 - Encoding (palavras)\n",
    "\n",
    "- Para representar as palavras, vamos criar um encoding com base na frequência apresentada no corpus.\n",
    "- Utilizaremos alguns números para representar determinadas semânticas:\n",
    "  - padding(\"pad\"), necessário para deixar todas as entradas com mesmo tamanho, terá o valor 0.\n",
    "  - start-of-sequence(\"sos\"), usado para identificar o ínicio de cada sentença, terá o número 1\n",
    "  - unknown, para marcar palavras desconhecidas, terá o número 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "p2aNLT_LEAaX"
   },
   "outputs": [],
   "source": [
    "SOS = \"<sos>\"\n",
    "PADDING = \"<pad>\"\n",
    "UNKNOWN = \"<ukn>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IzA2U6fi3NYb"
   },
   "outputs": [],
   "source": [
    "def add_padding_and_sos(data, columns):\n",
    "\n",
    "\n",
    "    for col in columns:\n",
    "        max_length = max(len(seq) for seq in data[col])\n",
    "        for index,row in data.iterrows():\n",
    "            document = [SOS] + row[col]\n",
    "            paddings = (max_length - len(row[col])) * [PADDING]\n",
    "            document += paddings\n",
    "            data.at[index,col] = document\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = add_padding_and_sos(df_copy, ['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwR2dRIFHVq9",
    "outputId": "47f6b5e7-a5d1-4fe7-b699-0beb0bf7738a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'rather', 'reduc', 'feder', 'subsidi', 'univers', 'place', 'leftist', 'institut', 'merci', 'free', 'market', 'obama', 'want', 'increas', 'amount', 'student', 'borrow', 'interest', 'govern', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(df_copy.loc[190,'sentence'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "212Rr3J3Ka06"
   },
   "source": [
    "- Construindo o vocabulário com base na frequência de aparecimento.\n",
    "  - *Obs*: note a colocação de determinadas tags logo no ínicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "-qkd_GWhnxrT"
   },
   "outputs": [],
   "source": [
    "def create_vocabulary(data, columns):\n",
    "  \"\"\" Retorna uma lookup table (StaticVocabularyTable) \"\"\"\n",
    "\n",
    "  vocab = Counter()\n",
    "  for col in columns:\n",
    "      for document in data[col]:\n",
    "          vocab.update(document)\n",
    "\n",
    "  words = tf.constant([PADDING] +\n",
    "                      [SOS] +\n",
    "                      [UNKNOWN] +\n",
    "                      [word for word in vocab.keys() if word != PADDING and word != SOS]\n",
    "  )\n",
    "  words_ids = tf.range(len(vocab)+1, dtype=tf.int64) # +1 porque unknown é o único não mapeado\n",
    "  vocab_init = tf.lookup.KeyValueTensorInitializer(words,words_ids)\n",
    "  num_oov_buckets = 10000 # bucket para palavras desconhecidas\n",
    "  vocab_table = tf.lookup.StaticVocabularyTable(vocab_init,num_oov_buckets)\n",
    "\n",
    "  return vocab_table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9aSx7Sx-Ph93"
   },
   "source": [
    "- Demonstração de como estão os encodings de algumas palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ALwPlNXIq83",
    "outputId": "d8c9daac-3953-4517-bd3c-33cde28ccd1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=int64, numpy=array([[2628,  113,  635]])>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_table = create_vocabulary(df_copy, columns=['sentence'])\n",
    "vocab_table.lookup(tf.constant([b'nice trump move'.split(b' ')]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vcuvVw23nxrT"
   },
   "source": [
    "- Convertendo os tokens para o índice na tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yS398UOqnxrT",
    "outputId": "fd7e00df-f53f-401d-a6dd-f9cfe5ad8e0b"
   },
   "outputs": [],
   "source": [
    "def convert_words_to_freq(data, vocab, columns):\n",
    "\n",
    "  for index, row in data.iterrows():\n",
    "\n",
    "    for col in columns:\n",
    "      doc = tf.constant(row[col])\n",
    "      doc_ided = vocab.lookup(doc).numpy() if len (doc) > 0 else []\n",
    "      data.at[index, col] = doc_ided\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>news_link</th>\n",
       "      <th>outlet</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>group_id</th>\n",
       "      <th>num_sent</th>\n",
       "      <th>Label_bias</th>\n",
       "      <th>Label_opinion</th>\n",
       "      <th>article</th>\n",
       "      <th>biased_words4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...</td>\n",
       "      <td>https://eu.usatoday.com/story/tech/2020/02/03/...</td>\n",
       "      <td>usa-today</td>\n",
       "      <td>elections-2020</td>\n",
       "      <td>center</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Biased</td>\n",
       "      <td>Somewhat factual but also opinionated</td>\n",
       "      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
       "      <td>[belated, birtherism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 18, 19, 20, 21, 22, 23, 24, 25, 24, 26, 27...</td>\n",
       "      <td>https://www.nbcnews.com/news/sports/women-s-te...</td>\n",
       "      <td>msnbc</td>\n",
       "      <td>sport</td>\n",
       "      <td>left</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Non-biased</td>\n",
       "      <td>Entirely factual</td>\n",
       "      <td>FRISCO, Texas — The increasingly bitter disput...</td>\n",
       "      <td>[bitter]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence   \n",
       "0  [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...  \\\n",
       "1  [1, 18, 19, 20, 21, 22, 23, 24, 25, 24, 26, 27...   \n",
       "\n",
       "                                           news_link     outlet   \n",
       "0  https://eu.usatoday.com/story/tech/2020/02/03/...  usa-today  \\\n",
       "1  https://www.nbcnews.com/news/sports/women-s-te...      msnbc   \n",
       "\n",
       "            topic    type  group_id  num_sent  Label_bias   \n",
       "0  elections-2020  center       1.0       1.0      Biased  \\\n",
       "1           sport    left       1.0       1.0  Non-biased   \n",
       "\n",
       "                           Label_opinion   \n",
       "0  Somewhat factual but also opinionated  \\\n",
       "1                       Entirely factual   \n",
       "\n",
       "                                             article          biased_words4  \n",
       "0  YouTube says no ‘deepfakes’ or ‘birther’ video...  [belated, birtherism]  \n",
       "1  FRISCO, Texas — The increasingly bitter disput...               [bitter]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy = convert_words_to_freq(df_copy, vocab_table, ['sentence'])\n",
    "df_copy.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "28mGJnjWQC82"
   },
   "source": [
    "### 2.4 - One-hot encoding para features categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ItB9GETSQB7v"
   },
   "outputs": [],
   "source": [
    "def cats_to_one_hot(data,column_names):\n",
    "  \n",
    "  df_encoded = pd.get_dummies(data[column_names]).astype(np.float32)\n",
    "  data = pd.concat([data,df_encoded],axis=1)\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 18, 19, 20, 21, 22, 23, 24, 25, 24, 26, 27, 28, 29, 30, 31, 32,\n",
       "       33, 34, 35, 36, 37, 38, 39, 40,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.loc[1,'sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hQ7UUyZpSsUs",
    "outputId": "cf1f97d5-15fc-4865-ad9b-e99287eff3f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>news_link</th>\n",
       "      <th>outlet</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>group_id</th>\n",
       "      <th>num_sent</th>\n",
       "      <th>Label_bias</th>\n",
       "      <th>Label_opinion</th>\n",
       "      <th>article</th>\n",
       "      <th>...</th>\n",
       "      <th>outlet_msnbc</th>\n",
       "      <th>outlet_reuters</th>\n",
       "      <th>outlet_usa-today</th>\n",
       "      <th>type_center</th>\n",
       "      <th>type_left</th>\n",
       "      <th>type_right</th>\n",
       "      <th>Label_opinion_Entirely factual</th>\n",
       "      <th>Label_opinion_Expresses writer’s opinion</th>\n",
       "      <th>Label_opinion_No agreement</th>\n",
       "      <th>Label_opinion_Somewhat factual but also opinionated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...</td>\n",
       "      <td>https://eu.usatoday.com/story/tech/2020/02/03/...</td>\n",
       "      <td>usa-today</td>\n",
       "      <td>elections-2020</td>\n",
       "      <td>center</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Biased</td>\n",
       "      <td>Somewhat factual but also opinionated</td>\n",
       "      <td>YouTube says no ‘deepfakes’ or ‘birther’ video...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 18, 19, 20, 21, 22, 23, 24, 25, 24, 26, 27...</td>\n",
       "      <td>https://www.nbcnews.com/news/sports/women-s-te...</td>\n",
       "      <td>msnbc</td>\n",
       "      <td>sport</td>\n",
       "      <td>left</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Non-biased</td>\n",
       "      <td>Entirely factual</td>\n",
       "      <td>FRISCO, Texas — The increasingly bitter disput...</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51...</td>\n",
       "      <td>https://www.alternet.org/2019/01/here-are-5-of...</td>\n",
       "      <td>alternet</td>\n",
       "      <td>immigration</td>\n",
       "      <td>left</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Biased</td>\n",
       "      <td>Expresses writer’s opinion</td>\n",
       "      <td>Speaking to the country for the first time fro...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence   \n",
       "0  [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...  \\\n",
       "1  [1, 18, 19, 20, 21, 22, 23, 24, 25, 24, 26, 27...   \n",
       "2  [1, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51...   \n",
       "\n",
       "                                           news_link     outlet   \n",
       "0  https://eu.usatoday.com/story/tech/2020/02/03/...  usa-today  \\\n",
       "1  https://www.nbcnews.com/news/sports/women-s-te...      msnbc   \n",
       "2  https://www.alternet.org/2019/01/here-are-5-of...   alternet   \n",
       "\n",
       "            topic    type  group_id  num_sent  Label_bias   \n",
       "0  elections-2020  center       1.0       1.0      Biased  \\\n",
       "1           sport    left       1.0       1.0  Non-biased   \n",
       "2     immigration    left       1.0       1.0      Biased   \n",
       "\n",
       "                           Label_opinion   \n",
       "0  Somewhat factual but also opinionated  \\\n",
       "1                       Entirely factual   \n",
       "2             Expresses writer’s opinion   \n",
       "\n",
       "                                             article  ... outlet_msnbc   \n",
       "0  YouTube says no ‘deepfakes’ or ‘birther’ video...  ...          0.0  \\\n",
       "1  FRISCO, Texas — The increasingly bitter disput...  ...          1.0   \n",
       "2  Speaking to the country for the first time fro...  ...          0.0   \n",
       "\n",
       "   outlet_reuters  outlet_usa-today  type_center  type_left  type_right   \n",
       "0             0.0               1.0          1.0        0.0         0.0  \\\n",
       "1             0.0               0.0          0.0        1.0         0.0   \n",
       "2             0.0               0.0          0.0        1.0         0.0   \n",
       "\n",
       "   Label_opinion_Entirely factual  Label_opinion_Expresses writer’s opinion   \n",
       "0                             0.0                                       0.0  \\\n",
       "1                             1.0                                       0.0   \n",
       "2                             0.0                                       1.0   \n",
       "\n",
       "   Label_opinion_No agreement   \n",
       "0                         0.0  \\\n",
       "1                         0.0   \n",
       "2                         0.0   \n",
       "\n",
       "   Label_opinion_Somewhat factual but also opinionated  \n",
       "0                                                1.0    \n",
       "1                                                0.0    \n",
       "2                                                0.0    \n",
       "\n",
       "[3 rows x 40 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy = cats_to_one_hot(df_copy, column_names=['topic','outlet','type','Label_opinion'])\n",
    "df_copy.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hu0eDi-4UE47"
   },
   "source": [
    "### 2.5 - Limpando features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "nufVpqv2Udwn"
   },
   "outputs": [],
   "source": [
    "def drop_columns(data, column_names):\n",
    "  return data.drop(column_names,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GmFBOZxZXBAJ",
    "outputId": "eb6b7298-abef-47cb-cd1e-924e1f2177d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1690, 38)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy = drop_columns(df_copy,column_names=['news_link','group_id'])\n",
    "df_copy.head(1)\n",
    "df_copy.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PSOuFsrwcT3t"
   },
   "source": [
    "### 2.7 - Compilando pipeline de pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "pED3BesynKbf"
   },
   "outputs": [],
   "source": [
    "class VocabularyConversionTransformer(BaseEstimator,TransformerMixin):\n",
    "  def __init__(self,vocab_columns, columns_to_be_converted):\n",
    "    self.vocab_table = None\n",
    "    self.columns_to_be_converted = columns_to_be_converted\n",
    "    self.vocab_columns = vocab_columns\n",
    "\n",
    "  def fit(self, X, y=None):\n",
    "    self.vocab_table = create_vocabulary(X,self.vocab_columns)\n",
    "    return self\n",
    "\n",
    "  def transform(self,X,y=None):\n",
    "    return convert_words_to_freq(X,columns=self.columns_to_be_converted,vocab=self.vocab_table)\n",
    "    \n",
    "  def get_vocabulary(self):\n",
    "    return self.vocab_table\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "FX_bDiJUcF91"
   },
   "outputs": [],
   "source": [
    "def preprocess(data,stemmer_flag=False,stemmer_type=None):\n",
    "\n",
    "  one_hot_columns = ['topic','outlet','type','Label_bias']\n",
    "  columns_to_drop = ['news_link','group_id', 'Label_opinion','num_sent'] + one_hot_columns\n",
    "  vocab_columns = ['article']\n",
    "  text_columns = ['article', 'sentence', 'biased_words4']\n",
    "\n",
    "  preprocess_pipeline = [\n",
    "      ('Tokenização', FunctionTransformer(preprocess_basic_text,kw_args={'columns': text_columns})),\n",
    "      ('Stemming', FunctionTransformer(apply_stemmer_for_all, kw_args={'columns':text_columns,\n",
    "                                                                      'stemmer_flag': stemmer_flag,\n",
    "                                                                      'stemmer_type': stemmer_type\n",
    "                                                                      })),\n",
    "      ('SOS/Padding Addition', FunctionTransformer(add_padding_and_sos, kw_args={'columns':text_columns})),\n",
    "      ('Vocab Creation and Word-Int Conversion', VocabularyConversionTransformer(vocab_columns=vocab_columns,columns_to_be_converted=text_columns)),\n",
    "      ('One-hot-encoding', FunctionTransformer(cats_to_one_hot,kw_args={'column_names':one_hot_columns})),\n",
    "      ('Dropping features', FunctionTransformer(drop_columns,kw_args={'column_names':columns_to_drop})),\n",
    "  ]\n",
    "\n",
    "  pipeline = Pipeline(preprocess_pipeline)\n",
    "\n",
    "  data = pipeline.fit_transform(data)\n",
    "\n",
    "  vocab = pipeline.named_steps['Vocab Creation and Word-Int Conversion'].get_vocabulary()\n",
    "\n",
    "  return data, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "PBRClwvPcTB0"
   },
   "outputs": [],
   "source": [
    "data, vocab = preprocess(df.copy(),True,stemmer_type=\"Porter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['sentence'].iloc[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rKHlDGUvKR9S"
   },
   "source": [
    "### 2.7 - Salvando dataset final (e vocabulário)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" def list_to_str(x):\\n    str = np.array2string(x, separator=',').replace('\\n', '')\\n    return str \""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def list_to_str(x):\n",
    "    str = np.array2string(x, separator=',').replace('\\n', '')\n",
    "    return str \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # A lista presente na coluna \"article\" é muito grande, então para convertê-la corretamente\\n# precisamos modificar uma configuração do Numpy\\nimport sys\\nnp.set_printoptions(threshold=sys.maxsize)\\n\\ndata.article.apply(list_to_str)\\ndata.sentence.apply(list_to_str)\\ndata.biased_words4.apply(list_to_str) '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # A lista presente na coluna \"article\" é muito grande, então para convertê-la corretamente\n",
    "# precisamos modificar uma configuração do Numpy\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "data.article.apply(list_to_str)\n",
    "data.sentence.apply(list_to_str)\n",
    "data.biased_words4.apply(list_to_str) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "EEWE1g73KaVx",
    "outputId": "8501473e-d373-426b-c659-846c37bd9786"
   },
   "outputs": [],
   "source": [
    "data.to_csv('../data/final_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = tf.constant(list(range(vocab.size())))\n",
    "keys = tf.strings.as_string(keys).numpy().tolist()\n",
    "values = [vocab.lookup(tf.constant(key)).numpy() for key in keys]\n",
    "\n",
    "vocab_dict = {key: value for key, value in zip(keys, values)}\n",
    "\n",
    "with open('../data/vocabulary_table.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab_dict, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais\n",
    "\n",
    "Esse notebook tratará da implementação, do treino e da avaliação inicial de redes neurais recorrentes (especificamente de células LSTM) na busca pela solução do seguinte problema de classificação: identificação de de viés político em textos. O tamanho desses textos é algo a ser arbitrado e até mesmo utilizado como comparação: podemos utilizar somente as sentenças que foram marcadas por conter viés, ou mesmo o texto na íntegra da notícia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from tensorflow import keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Carregamento do dataset e do vocabulário, totalmente perfilados pelo notebook de pré-processamento."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Separando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = ['Label_bias_Biased','Label_bias_No agreement', 'Label_bias_Non-biased']\n",
    "labels = data[label_columns]\n",
    "features = data.drop(label_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split(features, labels):\n",
    "    \"\"\" Retorna uma lista de tuplas contendo os datasets de features e de labels para cada segmento (treino, validação, teste) \"\"\"\n",
    "\n",
    "    # Treino-val e Teste\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(features, labels, test_size=0.2,stratify=labels, random_state=104)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, stratify=y_train_val)\n",
    "\n",
    "    return [(X_train, y_train), (X_val, y_val), (X_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino:(1081, 28) ----- Validação: (271, 28) ------ Teste: (338, 28)\n"
     ]
    }
   ],
   "source": [
    "train, validation, test = train_valid_test_split(features,labels)\n",
    "\n",
    "train_df, train_labels = train\n",
    "validation_df, validation_labels = validation\n",
    "test_df, test_labels = test\n",
    "\n",
    "print(f\"Treino:{train_df.shape} ----- Validação: {validation_df.shape} ------ Teste: {test_df.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Convertendo para formatos aceitos como input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tf_formats(data):\n",
    "\n",
    "    text_cols = ['sentence', 'biased_words4', 'article']\n",
    "\n",
    "    fn = lambda x: tf.convert_to_tensor(x,dtype=tf.int32)\n",
    "\n",
    "    for col in text_cols:        \n",
    "        data[col] = data[col].apply(fn)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = convert_to_tf_formats(train_df)\n",
    "validation_df = convert_to_tf_formats(validation_df)\n",
    "test_df = convert_to_tf_formats(test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Estruturando arquitetura da rede neural"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vamos criar uma camada de embeddings. No caso atual, só conseguimos utilizar uma feature do tipo texto por vez, tendo em vista que o embedding é treinado junto da rede como um todo. Uma das ideias futuras seria utilizar um modelo pré-treinado com o vocabulário reduzido ao nosso, mas para alguns testes, essa arquitetura será interessante. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Parte Textual (artigo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "\n",
    "text_input_length = len(train_df.loc[1,'article'])\n",
    "text_input = keras.layers.Input(shape=(text_input_length,))\n",
    "embedding_layer = keras.layers.Embedding(input_dim=int(vocab_table.size())+10000,\n",
    "                                         output_dim=embed_size,\n",
    "                                         mask_zero=True)(text_input)\n",
    "\n",
    "embedding_output = keras.layers.Bidirectional(keras.layers.LSTM(embed_size))(embedding_layer) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Parte de Metadados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_input_length = len(train_df.columns) - 3\n",
    "feature_input = keras.layers.Input(shape=(feature_input_length,))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 03:04:40.779908: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-26 03:04:40.782754: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-26 03:04:40.786308: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "concatenated = keras.layers.concatenate([embedding_output,feature_input], axis=-1)\n",
    "reshaped = keras.layers.Reshape((-1, concatenated.shape[-1]))(concatenated)  # Add time dimension\n",
    "gru_layer1 = keras.layers.GRU(128)(reshaped)\n",
    "output = keras.layers.Dense(3, activation=\"softmax\")(gru_layer1)\n",
    "\n",
    "model = tf.keras.Model(inputs=[text_input,feature_input], outputs=output)\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_22 (InputLayer)          [(None, 3000)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_18 (Embedding)       (None, 3000, 128)    2718592     ['input_22[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional_7 (Bidirectional  (None, 256)         263168      ['embedding_18[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_23 (InputLayer)          [(None, 25)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 281)          0           ['bidirectional_7[0][0]',        \n",
      "                                                                  'input_23[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_9 (Reshape)            (None, 1, 281)       0           ['concatenate_8[0][0]']          \n",
      "                                                                                                  \n",
      " gru_11 (GRU)                   (None, 128)          157824      ['reshape_9[0][0]']              \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 3)            387         ['gru_11[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,139,971\n",
      "Trainable params: 3,139,971\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Treinando a rede"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Função auxiliar para fragmentar os inputs por tipo de feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_metadata_and_text_features(data,text_column):\n",
    "\n",
    "    \"\"\" \n",
    "        Realiza a separação entre as features de texto e de metadados. \n",
    "        O parâmetro \"text_column\" diz qual será a coluna textual que utilizaremos, dado que \n",
    "        atualmente, precisamos escolher somente uma.\n",
    "    \"\"\"\n",
    "\n",
    "    data = data.copy()\n",
    "\n",
    "    text_features = tf.constant(data[text_column].values.tolist())\n",
    "    metadata_features = data.drop(['article','sentence','biased_words4'],axis=1)\n",
    "    \n",
    "    return [text_features, metadata_features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Callback para salvar modelo a cada epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"../data/article_model.h5\",\n",
    "    save_freq=\"epoch\",\n",
    "    save_best_only=False,\n",
    "    save_weights_only=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Treinamento efetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 03:17:39.566508: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-26 03:17:39.571870: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-26 03:17:39.574841: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-06-26 03:17:41.386757: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-06-26 03:17:44.310068: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-26 03:17:44.315412: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-26 03:17:44.319754: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-06-26 03:17:45.710237: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/17 [>.............................] - ETA: 9:05 - loss: 1.0859"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ruasgar/Bureau/text-bias-detection/code/preprocess.ipynb Cell 73\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ruasgar/Bureau/text-bias-detection/code/preprocess.ipynb#Y121sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(split_metadata_and_text_features(train_df,\u001b[39m'\u001b[39;49m\u001b[39marticle\u001b[39;49m\u001b[39m'\u001b[39;49m), \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ruasgar/Bureau/text-bias-detection/code/preprocess.ipynb#Y121sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m           train_labels, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ruasgar/Bureau/text-bias-detection/code/preprocess.ipynb#Y121sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m           epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ruasgar/Bureau/text-bias-detection/code/preprocess.ipynb#Y121sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m           validation_data\u001b[39m=\u001b[39;49m(split_metadata_and_text_features(validation_df,\u001b[39m'\u001b[39;49m\u001b[39marticle\u001b[39;49m\u001b[39m'\u001b[39;49m), validation_labels),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ruasgar/Bureau/text-bias-detection/code/preprocess.ipynb#Y121sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m           batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ruasgar/Bureau/text-bias-detection/code/preprocess.ipynb#Y121sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m           callbacks\u001b[39m=\u001b[39;49m[checkpoint_cb]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ruasgar/Bureau/text-bias-detection/code/preprocess.ipynb#Y121sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(split_metadata_and_text_features(train_df,'article'), \n",
    "          train_labels, \n",
    "          epochs=20,\n",
    "          validation_data=(split_metadata_and_text_features(validation_df,'article'), validation_labels),\n",
    "          batch_size=64, \n",
    "          callbacks=[checkpoint_cb]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "8jhbot7NnxrP",
    "5dgVKAMqnxrR",
    "KOOeFWainxrT",
    "28mGJnjWQC82",
    "hu0eDi-4UE47"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
