{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais\n",
    "\n",
    "Esse notebook tratará da implementação, do treino e da avaliação inicial de redes neurais recorrentes (especificamente de células LSTM) na busca pela solução do seguinte problema de classificação: identificação de de viés político em textos. O tamanho desses textos é algo a ser arbitrado e até mesmo utilizado como comparação: podemos utilizar somente as sentenças que foram marcadas por conter viés, ou mesmo o texto na íntegra da notícia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import ast\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit \n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Carregamento do dataset e do vocabulário, totalmente perfilados pelo notebook de pré-processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/final_dataset.csv')\n",
    "\n",
    "# transforma as listas que estão em string em listas novamente\n",
    "#data[\"sentence\"] = data[\"sentence\"].apply(eval)\n",
    "#data[\"article\"] = data[\"article\"].apply(eval)\n",
    "#data[\"biased_words4\"] = data[\"biased_words4\"].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma as listas em Arrays do Numpy\n",
    "\"\"\" data[\"sentence\"] = data[\"sentence\"].apply(np.asarray)\n",
    "data[\"article\"] = data[\"article\"].apply(np.asarray)\n",
    "data[\"biased_words4\"] = data[\"biased_words4\"].apply(np.asarray) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence', 'outlet', 'topic', 'type', 'num_sent', 'Label_bias',\n",
       "       'article', 'biased_words4', 'topic_abortion', 'topic_coronavirus',\n",
       "       'topic_elections-2020', 'topic_environment', 'topic_gender',\n",
       "       'topic_gun-control', 'topic_immigration',\n",
       "       'topic_international-politics-and-world-news', 'topic_middle-class',\n",
       "       'topic_sport', 'topic_student-debt', 'topic_trump-presidency',\n",
       "       'topic_vaccines', 'topic_white-nationalism', 'outlet_alternet',\n",
       "       'outlet_breitbart', 'outlet_federalist', 'outlet_fox-news',\n",
       "       'outlet_huffpost', 'outlet_msnbc', 'outlet_reuters', 'outlet_usa-today',\n",
       "       'type_center', 'type_left', 'type_right', 'Label_bias_Biased',\n",
       "       'Label_bias_No agreement', 'Label_bias_Non-biased'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/vocabulary_table.pkl', 'rb') as f:\n",
    "    vocab_dict = pickle.load(f)\n",
    "\n",
    "keys = list(vocab_dict.keys())\n",
    "values = list(vocab_dict.values())\n",
    "n_oov = 5000\n",
    "\n",
    "vocab_table = tf.lookup.StaticVocabularyTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(keys, values, tf.string, tf.int64),\n",
    "    n_oov\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Separando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = ['Label_bias_Biased','Label_bias_No agreement', 'Label_bias_Non-biased']\n",
    "labels = data[label_columns]\n",
    "features = data.drop(label_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split(features, labels):\n",
    "    \"\"\" Retorna uma lista de tuplas contendo os datasets de features e de labels para cada segmento (treino, validação, teste) \"\"\"\n",
    "\n",
    "    # Treino-val e Teste\n",
    "    shuffle_train_test = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=892)\n",
    "    train_val_indexes, test_indexes = next(shuffle_train_test.split(features.values, labels.values))\n",
    "    train_val_df, train_val_labels = features.iloc[train_val_indexes], labels.iloc[train_val_indexes]\n",
    "    test_df, test_labels = features.iloc[test_indexes], labels.iloc[test_indexes]\n",
    "\n",
    "    # Treino e Validação\n",
    "    shuffle_train_validate = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=124)\n",
    "    train_indexes, validation_indexes = next(shuffle_train_validate.split(train_val_df.values,train_val_labels.values))\n",
    "    train_df, train_labels = features.iloc[train_indexes], labels.iloc[train_indexes]\n",
    "    validation_df, validation_labels = features.iloc[validation_indexes], labels.iloc[validation_indexes]\n",
    "\n",
    "\n",
    "    return [(train_df, train_labels), (validation_df, validation_labels), (test_df, test_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino:(1014, 33) ----- Validação: (338, 33) ------ Teste: (338, 33)\n"
     ]
    }
   ],
   "source": [
    "train, validation, test = train_valid_test_split(features,labels)\n",
    "\n",
    "train_df, train_labels = train\n",
    "validation_df, validation_labels = validation\n",
    "test_df, test_labels = test\n",
    "\n",
    "print(f\"Treino:{train_df.shape} ----- Validação: {validation_df.shape} ------ Teste: {test_df.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Estruturando arquitetura da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-25 20:27:50.142953: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 18352128 exceeds 10% of free system memory.\n",
      "2023-06-25 20:27:50.176498: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 18352128 exceeds 10% of free system memory.\n",
      "2023-06-25 20:27:50.187391: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 18352128 exceeds 10% of free system memory.\n",
      "2023-06-25 20:27:52.981225: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-25 20:27:52.983512: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-25 20:27:52.985055: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "\n",
    "# parte textual\n",
    "text_input_length = len(train_df.loc[0,'article']) + len(train_df.loc[0,'biased_words4']) + len(train_df.loc[0,'sentence'])\n",
    "text_input = keras.layers.Input(shape=(text_input_length,))\n",
    "embedding_layer = keras.layers.Embedding(input_dim=int(vocab_table.size())+5000,\n",
    "                                         output_dim=embed_size,\n",
    "                                         mask_zero=True)(text_input)\n",
    "\n",
    "embedding_output = keras.layers.Bidirectional(keras.layers.LSTM(embed_size))(embedding_layer) \n",
    "\n",
    "\n",
    "# outras features \n",
    "feature_input_length = len(train_df.columns) - 3\n",
    "feature_input = keras.layers.Input(shape=(feature_input_length,))\n",
    "\n",
    "concatenated = keras.layers.concatenate([embedding_output,feature_input], axis=-1)\n",
    "reshaped = keras.layers.Reshape((-1, concatenated.shape[-1]))(concatenated)  # Add time dimension\n",
    "gru_layer1 = keras.layers.GRU(64)(reshaped)\n",
    "output = keras.layers.Dense(3, activation=\"softmax\")(gru_layer1)\n",
    "\n",
    "model = tf.keras.Model(inputs=[text_input,feature_input], outputs=output)\n",
    "#model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 12369)]      0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 12369, 128)   4588032     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 256)          263168      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 286)          0           ['bidirectional[0][0]',          \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 286)       0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " gru (GRU)                      (None, 64)           67584       ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3)            195         ['gru[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,918,979\n",
      "Trainable params: 4,918,979\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = ['sentence','article','biased_words4']\n",
    "others = list(filter(lambda x: x if x not in text_cols else [],train_df.columns))\n",
    "\n",
    "train_df = pd.concat([train_df[text_cols], train_df[others]],axis=1)\n",
    "validation_df = pd.concat([validation_df[text_cols], validation_df[others]],axis=1)\n",
    "test_df = pd.concat([test_df[text_cols], test_df[others]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93      [    1,13361,  843, 5621,  172, 6203,  408,  5...\n",
       "526     [   1,1434,1039,2681,1397, 701, 281, 304,1367,...\n",
       "761     [    1, 1472,   80, 3802,  191,  701,  254, 43...\n",
       "686     [    1, 4711,12583, 3519,  489,14024, 2248,  4...\n",
       "1067    [    1,12645,  145,  154,  145, 2301, 1043,   ...\n",
       "                              ...                        \n",
       "1047    [    1,  888, 1787, 3796,  966,13877,13878,173...\n",
       "1327    [    1, 9015, 1567, 3828, 1678,17015,  644, 23...\n",
       "1302    [   1, 522,1081, 458,4056,6374, 304,3236,1629,...\n",
       "245     [   1, 864, 776, 421, 168, 265,4140,3470,1775,...\n",
       "1000    [    1,  248, 1114, 3765, 3761,17010,  145,  5...\n",
       "Name: sentence, Length: 1014, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ruasgar/Bureau/text-bias-detection/code/network.ipynb Cell 17\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ruasgar/Bureau/text-bias-detection/code/network.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_df, train_labels, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(validation_df, validation_labels))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "model.fit(train_df, train_labels, batch_size=32, epochs=10, validation_data=(validation_df, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
